{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UImaOUytIfZv"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxPdeR9XIfZw"
   },
   "source": [
    "Training a Classifier\n",
    "=====================\n",
    "\n",
    "This is it. You have seen how to define neural networks, compute loss\n",
    "and make updates to the weights of the network.\n",
    "\n",
    "Now you might be thinking,\n",
    "\n",
    "What about data?\n",
    "----------------\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data,\n",
    "you can use standard python packages that load data into a numpy array.\n",
    "Then you can convert this array into a `torch.*Tensor`.\n",
    "\n",
    "-   For images, packages such as Pillow, OpenCV are useful\n",
    "-   For audio, packages such as scipy and librosa\n",
    "-   For text, either raw Python or Cython based loading, or NLTK and\n",
    "    SpaCy are useful\n",
    "\n",
    "Specifically for vision, we have created a package called `torchvision`,\n",
    "that has data loaders for common datasets such as ImageNet, CIFAR10,\n",
    "MNIST, etc. and data transformers for images, viz.,\n",
    "`torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "This provides a huge convenience and avoids writing boilerplate code.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset. It has the classes:\n",
    "'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "'ship', 'truck'. The images in CIFAR-10 are of size 3x32x32, i.e.\n",
    "3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "![cifar10](https://pytorch.org/tutorials/_static/img/cifar10.png)\n",
    "\n",
    "Training an image classifier\n",
    "----------------------------\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1.  Load and normalize the CIFAR10 training and test datasets using\n",
    "    `torchvision`\n",
    "2.  Define a Convolutional Neural Network\n",
    "3.  Define a loss function\n",
    "4.  Train the network on the training data\n",
    "5.  Test the network on the test data\n",
    "\n",
    "### 1. Load and normalize CIFAR10\n",
    "\n",
    "Using `torchvision`, it's extremely easy to load CIFAR10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sbltT900IfZx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1sve6JZASli",
    "outputId": "56039e1e-c990-469f-8af3-ab5e6c867cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 31 19:36:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GB10                    On  |   0000000F:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8              4W /  N/A  | Not Supported          |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2433      G   /usr/lib/xorg/Xorg                       18MiB |\n",
      "|    0   N/A  N/A            2620      G   /usr/bin/gnome-shell                      6MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.22.3-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click>=8.0.1 in ./.venv/lib/python3.12/site-packages (from wandb) (8.3.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.12/site-packages (from wandb) (4.5.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_aarch64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic<3 in ./.venv/lib/python3.12/site-packages (from wandb) (2.12.3)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.venv/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.43.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in ./.venv/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.22.3-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (18.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_aarch64.whl (324 kB)\n",
      "Using cached sentry_sdk-2.43.0-py2.py3-none-any.whl (400 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, sentry-sdk, protobuf, gitdb, gitpython, wandb\n",
      "Successfully installed gitdb-4.0.12 gitpython-3.1.45 protobuf-6.33.0 sentry-sdk-2.43.0 smmap-5.0.2 wandb-0.22.3\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXhqXMkdIfZx"
   },
   "source": [
    "The output of torchvision datasets are PILImage images of range \\[0,\n",
    "1\\]. We transform them to Tensors of normalized range \\[-1, 1\\].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an5WcqxDIfZx"
   },
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>If you are running this tutorial on Windows or MacOS and encounter aBrokenPipeError or RuntimeError related to multiprocessing, try settingthe num_worker of torch.utils.data.DataLoader() to 0.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QL8Fu4miIfZx",
    "outputId": "4cc703c6-0fee-4c45-fce3-591ef7684f09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:05<00:00, 33.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), # Converts a PIL Image or numpy.ndarray to a PyTorch Tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Normalizes a tensor image with mean and standard deviation.\n",
    "\n",
    "batch_size = 4 # Define the batch size for the data loaders\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, # Load the CIFAR10 training dataset\n",
    "                                        download=True, transform=transform) # Download the dataset if not already present and apply the defined transformations\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, # Create a DataLoader for the training dataset\n",
    "                                          shuffle=True, num_workers=2) # Shuffle the data and use 2 worker processes for loading\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, # Load the CIFAR10 test dataset\n",
    "                                       download=True, transform=transform) # Download the dataset if not already present and apply the defined transformations\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, # Create a DataLoader for the test dataset\n",
    "                                         shuffle=False, num_workers=2) # Do not shuffle the test data and use 2 worker processes for loading\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', # Define the class names in the CIFAR10 dataset\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfd3bf-LIfZx"
   },
   "source": [
    "Let us show some of the training images, for fun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "QTSA8HUZIfZy",
    "outputId": "5089dc12-eebd-47b0-bd86-35b57f0b6abe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse frog  deer  ship \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0MvHYWnIfZy"
   },
   "source": [
    "2. Define a Convolutional Neural Network\n",
    "========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bE2ErUPLIfZy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Training Framework with Performance Monitoring\n",
    "Author: Hochan Son\n",
    "Course: STATS 413 HW3\n",
    "\n",
    "This module provides a structured framework for training CNN models on CIFAR-10\n",
    "with built-in performance metrics monitoring and logging to Weights & Biases.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "import wandb\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration class for training hyperparameters.\"\"\"\n",
    "    learning_rate: float = 0.001\n",
    "    momentum: float = 0.9\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 4\n",
    "    num_workers: int = 2\n",
    "    device: str = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    log_interval: int = 2000\n",
    "\n",
    "\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Base class for monitoring and logging performance metrics during training.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initialize performance metrics tracker.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name identifier for the model\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.training_losses: List[float] = []\n",
    "        self.epoch_times: List[float] = []\n",
    "        self.test_accuracy: Optional[float] = None\n",
    "        self.class_accuracies: Dict[str, float] = {}\n",
    "        self.start_time: Optional[float] = None\n",
    "        self.total_training_time: float = 0.0\n",
    "\n",
    "    def start_epoch_timer(self):\n",
    "        \"\"\"Start timer for epoch duration tracking.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def end_epoch_timer(self) -> float:\n",
    "        \"\"\"End timer and record epoch duration.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            return 0.0\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_times.append(elapsed)\n",
    "        self.total_training_time += elapsed\n",
    "        self.start_time = None\n",
    "        return elapsed\n",
    "\n",
    "    def log_training_loss(self, loss: float, step: int):\n",
    "        \"\"\"\n",
    "        Log training loss.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss value to log\n",
    "            step: Current training step\n",
    "        \"\"\"\n",
    "        self.training_losses.append(loss)\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({f\"{self.model_name}/training_loss\": loss}, step=step)\n",
    "\n",
    "    def log_test_results(self, overall_accuracy: float, class_accuracies: Dict[str, float]):\n",
    "        \"\"\"\n",
    "        Log test evaluation results.\n",
    "\n",
    "        Args:\n",
    "            overall_accuracy: Overall test accuracy percentage\n",
    "            class_accuracies: Per-class accuracy percentages\n",
    "        \"\"\"\n",
    "        self.test_accuracy = overall_accuracy\n",
    "        self.class_accuracies = class_accuracies\n",
    "\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                f\"{self.model_name}/test_overall_accuracy\": overall_accuracy,\n",
    "                f\"{self.model_name}/total_training_time\": self.total_training_time\n",
    "            })\n",
    "\n",
    "            for classname, accuracy in class_accuracies.items():\n",
    "                wandb.log({f\"{self.model_name}/test_accuracy_{classname}\": accuracy})\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a summary of all collected metrics.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Performance Summary for {self.model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total Training Time: {self.total_training_time:.2f} seconds\")\n",
    "        print(f\"Average Epoch Time: {sum(self.epoch_times)/len(self.epoch_times):.2f} seconds\")\n",
    "        print(f\"Overall Test Accuracy: {self.test_accuracy:.2f}%\")\n",
    "        print(f\"\\nPer-Class Accuracies:\")\n",
    "        for classname, accuracy in self.class_accuracies.items():\n",
    "            print(f\"  {classname:10s}: {accuracy:5.1f}%\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"Base class for CNN models with integrated performance monitoring.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initialize CNN model.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name identifier for the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.metrics = PerformanceMetrics(model_name)\n",
    "\n",
    "    def get_optimizer(self, config: TrainingConfig) -> optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Get optimizer for training. Can be overridden by subclasses.\n",
    "\n",
    "        Args:\n",
    "            config: Training configuration\n",
    "\n",
    "        Returns:\n",
    "            PyTorch optimizer instance\n",
    "        \"\"\"\n",
    "        return optim.SGD(self.parameters(), lr=config.learning_rate, momentum=config.momentum)\n",
    "\n",
    "    def train_model(self, trainloader, config: TrainingConfig, criterion):\n",
    "        \"\"\"\n",
    "        Train the model on the training dataset.\n",
    "\n",
    "        Args:\n",
    "            trainloader: DataLoader for training data\n",
    "            config: Training configuration\n",
    "            criterion: Loss function\n",
    "        \"\"\"\n",
    "        optimizer = self.get_optimizer(config)\n",
    "        device = torch.device(config.device)\n",
    "        self.to(device)\n",
    "\n",
    "        print(f\"\\nTraining {self.model_name}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            self.metrics.start_epoch_timer()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if i % config.log_interval == (config.log_interval - 1):\n",
    "                    avg_loss = running_loss / config.log_interval\n",
    "                    step = epoch * len(trainloader) + i\n",
    "                    print(f'[Epoch {epoch + 1:2d}, Batch {i + 1:5d}] loss: {avg_loss:.3f}')\n",
    "                    self.metrics.log_training_loss(avg_loss, step)\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            epoch_time = self.metrics.end_epoch_timer()\n",
    "            print(f'Epoch {epoch + 1} completed in {epoch_time:.2f} seconds')\n",
    "\n",
    "        print(f'Finished Training {self.model_name}')\n",
    "\n",
    "    def evaluate_model(self, testloader, classes: Tuple[str, ...], config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test dataset.\n",
    "\n",
    "        Args:\n",
    "            testloader: DataLoader for test data\n",
    "            classes: Tuple of class names\n",
    "            config: Training configuration\n",
    "        \"\"\"\n",
    "        device = torch.device(config.device)\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "\n",
    "        print(f\"\\nEvaluating {self.model_name}...\")\n",
    "\n",
    "        # Overall accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        overall_accuracy = 100 * correct / total\n",
    "\n",
    "        # Per-class accuracy\n",
    "        correct_pred = {classname: 0 for classname in classes}\n",
    "        total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = self(images)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "                for label, prediction in zip(labels, predictions):\n",
    "                    if label == prediction:\n",
    "                        correct_pred[classes[label]] += 1\n",
    "                    total_pred[classes[label]] += 1\n",
    "\n",
    "        class_accuracies = {\n",
    "            classname: 100 * float(correct_pred[classname]) / total_pred[classname]\n",
    "            for classname in classes\n",
    "        }\n",
    "\n",
    "        self.metrics.log_test_results(overall_accuracy, class_accuracies)\n",
    "        self.metrics.print_summary()\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Save model state dictionary.\n",
    "\n",
    "        Args:\n",
    "            path: File path to save the model\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model {self.model_name} saved to {path}\")\n",
    "\n",
    "\n",
    "class OriginalNet(CNNModel):\n",
    "    \"\"\"Original baseline CNN architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"OriginalNet\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DoubleNet_1(CNNModel):\n",
    "    \"\"\"CNN with doubled channel dimensions.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"DoubleNet_1\")\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 240)\n",
    "        self.fc2 = nn.Linear(240, 168)\n",
    "        self.fc3 = nn.Linear(168, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DoubleNet_2(CNNModel):\n",
    "    \"\"\"CNN with doubled channels and additional convolutional layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"DoubleNet_2\")\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        # After conv1 (32->28) + pool (28->14)\n",
    "        # After conv2 (14->10) + pool (10->5)\n",
    "        # Shape is 32 * 5 * 5 = 800\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 240)\n",
    "        self.fc2 = nn.Linear(240, 168)\n",
    "        self.fc3 = nn.Linear(168, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Note: conv3 is defined but not used in forward pass to match original notebook\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def get_optimizer(self, config: TrainingConfig) -> optim.Optimizer:\n",
    "        \"\"\"Override to use AdamW optimizer instead of SGD.\"\"\"\n",
    "        return optim.AdamW(self.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_data(config: TrainingConfig) -> Tuple[torch.utils.data.DataLoader,\n",
    "                                                    torch.utils.data.DataLoader,\n",
    "                                                    Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Prepare CIFAR-10 data loaders and class names.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (trainloader, testloader, classes)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate model training and evaluation.\"\"\"\n",
    "\n",
    "    # Initialize configuration\n",
    "    config = TrainingConfig(\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        epochs=20,\n",
    "        batch_size=4,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"STATS413_HW3\",\n",
    "        #api_key=\"13d55e02e607f6f881f5e8cecfe06fd17a7dc672\",\n",
    "        entity=\"ohsono-ucla\",\n",
    "        config={\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"momentum\": config.momentum,\n",
    "            \"epochs\": config.epochs,\n",
    "            \"batch_size\": config.batch_size\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check device\n",
    "    device = torch.device(config.device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    trainloader, testloader, classes = prepare_data(config)\n",
    "    print(f\"Data loaded successfully. Training samples: {len(trainloader.dataset)}, \"\n",
    "          f\"Test samples: {len(testloader.dataset)}\")\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize models\n",
    "    models = [\n",
    "        # OriginalNet(),\n",
    "         DoubleNet_1(),\n",
    "        #DoubleNet_2()\n",
    "    ]\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model in models:\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"Processing Model: {model.model_name}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "\n",
    "        # Train\n",
    "        model.train_model(trainloader, config, criterion)\n",
    "\n",
    "        # Save model\n",
    "        save_path = f'./data/cifar_{model.model_name}.pth'\n",
    "        model.save_model(save_path)\n",
    "\n",
    "        # Evaluate\n",
    "        model.evaluate_model(testloader, classes, config)\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(\"All models trained and evaluated successfully!\")\n",
    "    print(f\"{'#'*60}\")\n",
    "\n",
    "    # Close wandb\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLUnet / TanhNet / ResidualBlock / ResNet\n",
    "1) LeakyReLUNet: CNN architecture with LeakyReLU activation\n",
    "2) TanhNet: CNN architecture with Tanh activation\n",
    "3) ResidualBlock: Residual block with identity connection\n",
    "4) ResNet: ResNet architecture with residual blocks for CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLUNet(CNNModel):\n",
    "    \"\"\"CNN architecture with LeakyReLU activation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"CNN_LeakyReLU\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.leaky_relu(self.conv1(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TanhNet(CNNModel):\n",
    "    \"\"\"CNN architecture with Tanh activation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"CNN_Tanh\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.tanh(self.conv1(x)))\n",
    "        x = self.pool(torch.tanh(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with identity connection.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv2d -> BatchNorm -> ReLU -> Conv2d -> BatchNorm\n",
    "    - Skip connection (with 1x1 conv if channels change)\n",
    "    - Final ReLU after adding residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        \"\"\"\n",
    "        Initialize residual block.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            stride: Stride for convolution (used for downsampling)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # Use 1x1 conv to match dimensions\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        out += identity  # Add skip connection\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(CNNModel):\n",
    "    \"\"\"\n",
    "    ResNet architecture with residual blocks for CIFAR-10.\n",
    "    \n",
    "    Network structure:\n",
    "    - Initial conv layer (3x32x32 -> 16x32x32)\n",
    "    - Residual block 1 (16x32x32 -> 32x16x16) with stride=2\n",
    "    - Residual block 2 (32x16x16 -> 64x8x8) with stride=2\n",
    "    - Residual block 3 (64x8x8 -> 64x8x8)\n",
    "    - Average pooling -> FC layer\n",
    "    \n",
    "    This design uses fewer parameters than very deep ResNets but\n",
    "    demonstrates the residual connection concept.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"ResNet_Custom\")\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = ResidualBlock(16, 32, stride=2)  # Downsample\n",
    "        self.layer2 = ResidualBlock(32, 64, stride=2)  # Downsample\n",
    "        self.layer3 = ResidualBlock(64, 64, stride=1)  # Same size\n",
    "        \n",
    "        # Global average pooling and FC\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # Global pooling and FC\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ohsono/jupyterlab/wandb/run-20251031_204647-cvjzl03c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations/runs/cvjzl03c' target=\"_blank\">rose-aardvark-1</a></strong> to <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations/runs/cvjzl03c' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations/runs/cvjzl03c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Preparing data...\n",
      "Data loaded successfully. Training samples: 50000, Test samples: 10000\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: Comparing Different Activation Functions\n",
      "================================================================================\n",
      "\n",
      "Models to compare:\n",
      "1. OriginalNet_ReLU: Baseline with ReLU activation\n",
      "2. CNN_LeakyReLU: LeakyReLU with negative_slope=0.01\n",
      "3. CNN_Tanh: Tanh activation\n",
      "\n",
      "All models use the same architecture (conv-pool-conv-pool-fc-fc-fc)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Processing Model: OriginalNet\n",
      "################################################################################\n",
      "\n",
      "Training OriginalNet...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.160\n",
      "[Epoch  1, Batch  4000] loss: 1.794\n",
      "[Epoch  1, Batch  6000] loss: 1.673\n",
      "[Epoch  1, Batch  8000] loss: 1.574\n",
      "[Epoch  1, Batch 10000] loss: 1.509\n",
      "[Epoch  1, Batch 12000] loss: 1.459\n",
      "Epoch 1 completed in 32.65 seconds\n",
      "[Epoch  2, Batch  2000] loss: 1.388\n",
      "[Epoch  2, Batch  4000] loss: 1.385\n",
      "[Epoch  2, Batch  6000] loss: 1.370\n",
      "[Epoch  2, Batch  8000] loss: 1.312\n",
      "[Epoch  2, Batch 10000] loss: 1.311\n",
      "[Epoch  2, Batch 12000] loss: 1.294\n",
      "Epoch 2 completed in 25.32 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.215\n",
      "[Epoch  3, Batch  4000] loss: 1.251\n",
      "[Epoch  3, Batch  6000] loss: 1.217\n",
      "[Epoch  3, Batch  8000] loss: 1.205\n",
      "[Epoch  3, Batch 10000] loss: 1.177\n",
      "[Epoch  3, Batch 12000] loss: 1.174\n",
      "Epoch 3 completed in 34.91 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.114\n",
      "[Epoch  4, Batch  4000] loss: 1.119\n",
      "[Epoch  4, Batch  6000] loss: 1.120\n",
      "[Epoch  4, Batch  8000] loss: 1.098\n",
      "[Epoch  4, Batch 10000] loss: 1.111\n",
      "[Epoch  4, Batch 12000] loss: 1.121\n",
      "Epoch 4 completed in 23.26 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.022\n",
      "[Epoch  5, Batch  4000] loss: 1.027\n",
      "[Epoch  5, Batch  6000] loss: 1.044\n",
      "[Epoch  5, Batch  8000] loss: 1.048\n",
      "[Epoch  5, Batch 10000] loss: 1.039\n",
      "[Epoch  5, Batch 12000] loss: 1.050\n",
      "Epoch 5 completed in 37.03 seconds\n",
      "[Epoch  6, Batch  2000] loss: 0.948\n",
      "[Epoch  6, Batch  4000] loss: 0.983\n",
      "[Epoch  6, Batch  6000] loss: 0.979\n",
      "[Epoch  6, Batch  8000] loss: 1.001\n",
      "[Epoch  6, Batch 10000] loss: 0.982\n",
      "[Epoch  6, Batch 12000] loss: 0.991\n",
      "Epoch 6 completed in 22.49 seconds\n",
      "[Epoch  7, Batch  2000] loss: 0.898\n",
      "[Epoch  7, Batch  4000] loss: 0.909\n",
      "[Epoch  7, Batch  6000] loss: 0.918\n",
      "[Epoch  7, Batch  8000] loss: 0.925\n",
      "[Epoch  7, Batch 10000] loss: 0.995\n",
      "[Epoch  7, Batch 12000] loss: 0.950\n",
      "Epoch 7 completed in 26.32 seconds\n",
      "[Epoch  8, Batch  2000] loss: 0.852\n",
      "[Epoch  8, Batch  4000] loss: 0.871\n",
      "[Epoch  8, Batch  6000] loss: 0.877\n",
      "[Epoch  8, Batch  8000] loss: 0.903\n",
      "[Epoch  8, Batch 10000] loss: 0.912\n",
      "[Epoch  8, Batch 12000] loss: 0.923\n",
      "Epoch 8 completed in 29.99 seconds\n",
      "[Epoch  9, Batch  2000] loss: 0.805\n",
      "[Epoch  9, Batch  4000] loss: 0.849\n",
      "[Epoch  9, Batch  6000] loss: 0.843\n",
      "[Epoch  9, Batch  8000] loss: 0.866\n",
      "[Epoch  9, Batch 10000] loss: 0.881\n",
      "[Epoch  9, Batch 12000] loss: 0.878\n",
      "Epoch 9 completed in 35.94 seconds\n",
      "[Epoch 10, Batch  2000] loss: 0.780\n",
      "[Epoch 10, Batch  4000] loss: 0.818\n",
      "[Epoch 10, Batch  6000] loss: 0.816\n",
      "[Epoch 10, Batch  8000] loss: 0.818\n",
      "[Epoch 10, Batch 10000] loss: 0.846\n",
      "[Epoch 10, Batch 12000] loss: 0.840\n",
      "Epoch 10 completed in 19.06 seconds\n",
      "[Epoch 11, Batch  2000] loss: 0.741\n",
      "[Epoch 11, Batch  4000] loss: 0.766\n",
      "[Epoch 11, Batch  6000] loss: 0.780\n",
      "[Epoch 11, Batch  8000] loss: 0.807\n",
      "[Epoch 11, Batch 10000] loss: 0.807\n",
      "[Epoch 11, Batch 12000] loss: 0.859\n",
      "Epoch 11 completed in 29.55 seconds\n",
      "[Epoch 12, Batch  2000] loss: 0.709\n",
      "[Epoch 12, Batch  4000] loss: 0.745\n",
      "[Epoch 12, Batch  6000] loss: 0.776\n",
      "[Epoch 12, Batch  8000] loss: 0.801\n",
      "[Epoch 12, Batch 10000] loss: 0.812\n",
      "[Epoch 12, Batch 12000] loss: 0.820\n",
      "Epoch 12 completed in 31.28 seconds\n",
      "[Epoch 13, Batch  2000] loss: 0.687\n",
      "[Epoch 13, Batch  4000] loss: 0.728\n",
      "[Epoch 13, Batch  6000] loss: 0.763\n",
      "[Epoch 13, Batch  8000] loss: 0.755\n",
      "[Epoch 13, Batch 10000] loss: 0.769\n",
      "[Epoch 13, Batch 12000] loss: 0.815\n",
      "Epoch 13 completed in 23.85 seconds\n",
      "[Epoch 14, Batch  2000] loss: 0.650\n",
      "[Epoch 14, Batch  4000] loss: 0.741\n",
      "[Epoch 14, Batch  6000] loss: 0.726\n",
      "[Epoch 14, Batch  8000] loss: 0.750\n",
      "[Epoch 14, Batch 10000] loss: 0.759\n",
      "[Epoch 14, Batch 12000] loss: 0.784\n",
      "Epoch 14 completed in 38.08 seconds\n",
      "[Epoch 15, Batch  2000] loss: 0.649\n",
      "[Epoch 15, Batch  4000] loss: 0.688\n",
      "[Epoch 15, Batch  6000] loss: 0.703\n",
      "[Epoch 15, Batch  8000] loss: 0.740\n",
      "[Epoch 15, Batch 10000] loss: 0.760\n",
      "[Epoch 15, Batch 12000] loss: 0.754\n",
      "Epoch 15 completed in 28.42 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.621\n",
      "[Epoch 16, Batch  4000] loss: 0.692\n",
      "[Epoch 16, Batch  6000] loss: 0.698\n",
      "[Epoch 16, Batch  8000] loss: 0.734\n",
      "[Epoch 16, Batch 10000] loss: 0.741\n",
      "[Epoch 16, Batch 12000] loss: 0.732\n",
      "Epoch 16 completed in 19.99 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.637\n",
      "[Epoch 17, Batch  4000] loss: 0.700\n",
      "[Epoch 17, Batch  6000] loss: 0.674\n",
      "[Epoch 17, Batch  8000] loss: 0.714\n",
      "[Epoch 17, Batch 10000] loss: 0.721\n",
      "[Epoch 17, Batch 12000] loss: 0.716\n",
      "Epoch 17 completed in 30.80 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.618\n",
      "[Epoch 18, Batch  4000] loss: 0.642\n",
      "[Epoch 18, Batch  6000] loss: 0.683\n",
      "[Epoch 18, Batch  8000] loss: 0.692\n",
      "[Epoch 18, Batch 10000] loss: 0.746\n",
      "[Epoch 18, Batch 12000] loss: 0.747\n",
      "Epoch 18 completed in 32.19 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.582\n",
      "[Epoch 19, Batch  4000] loss: 0.650\n",
      "[Epoch 19, Batch  6000] loss: 0.662\n",
      "[Epoch 19, Batch  8000] loss: 0.696\n",
      "[Epoch 19, Batch 10000] loss: 0.691\n",
      "[Epoch 19, Batch 12000] loss: 0.717\n",
      "Epoch 19 completed in 23.86 seconds\n",
      "[Epoch 20, Batch  2000] loss: 0.594\n",
      "[Epoch 20, Batch  4000] loss: 0.644\n",
      "[Epoch 20, Batch  6000] loss: 0.664\n",
      "[Epoch 20, Batch  8000] loss: 0.666\n",
      "[Epoch 20, Batch 10000] loss: 0.703\n",
      "[Epoch 20, Batch 12000] loss: 0.694\n",
      "Epoch 20 completed in 38.14 seconds\n",
      "Finished Training OriginalNet\n",
      "Model OriginalNet saved to ./data/cifar_OriginalNet.pth\n",
      "\n",
      "Evaluating OriginalNet...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for OriginalNet\n",
      "============================================================\n",
      "Total Training Time: 583.15 seconds\n",
      "Average Epoch Time: 29.16 seconds\n",
      "Overall Test Accuracy: 61.07%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  69.1%\n",
      "  car       :  75.7%\n",
      "  bird      :  55.9%\n",
      "  cat       :  45.5%\n",
      "  deer      :  57.6%\n",
      "  dog       :  44.2%\n",
      "  frog      :  60.3%\n",
      "  horse     :  63.6%\n",
      "  ship      :  68.6%\n",
      "  truck     :  70.2%\n",
      "============================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Processing Model: CNN_LeakyReLU\n",
      "################################################################################\n",
      "\n",
      "Training CNN_LeakyReLU...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1, Batch  4000] loss: 1.820\n",
      "[Epoch  1, Batch  6000] loss: 1.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1, Batch  8000] loss: 1.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1, Batch 10000] loss: 1.512\n",
      "[Epoch  1, Batch 12000] loss: 1.475\n",
      "Epoch 1 completed in 37.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2, Batch  2000] loss: 1.392\n",
      "[Epoch  2, Batch  4000] loss: 1.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 14499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 16499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2, Batch  6000] loss: 1.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 18499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2, Batch  8000] loss: 1.325\n",
      "[Epoch  2, Batch 10000] loss: 1.318\n",
      "[Epoch  2, Batch 12000] loss: 1.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 22499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 24499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed in 32.27 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.221\n",
      "[Epoch  3, Batch  4000] loss: 1.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 26999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 28999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch  6000] loss: 1.220\n",
      "[Epoch  3, Batch  8000] loss: 1.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 32999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch 10000] loss: 1.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 34999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch 12000] loss: 1.171\n",
      "Epoch 3 completed in 37.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 36999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch  2000] loss: 1.097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 39499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch  4000] loss: 1.118\n",
      "[Epoch  4, Batch  6000] loss: 1.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 41499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 43499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch  8000] loss: 1.132\n",
      "[Epoch  4, Batch 10000] loss: 1.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 45499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 47499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch 12000] loss: 1.101\n",
      "Epoch 4 completed in 36.15 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 49499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 51999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch  4000] loss: 1.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 53999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch  6000] loss: 1.048\n",
      "[Epoch  5, Batch  8000] loss: 1.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 55999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 57999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch 10000] loss: 1.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 59999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch 12000] loss: 1.062\n",
      "Epoch 5 completed in 41.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 61999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch  2000] loss: 0.960\n",
      "[Epoch  6, Batch  4000] loss: 0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 64499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 66499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch  6000] loss: 0.990\n",
      "[Epoch  6, Batch  8000] loss: 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 68499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch 10000] loss: 1.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 72499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch 12000] loss: 0.984\n",
      "Epoch 6 completed in 35.21 seconds\n",
      "[Epoch  7, Batch  2000] loss: 0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 74499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 76999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  7, Batch  4000] loss: 0.928\n",
      "[Epoch  7, Batch  6000] loss: 0.947\n",
      "[Epoch  7, Batch  8000] loss: 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 78999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 82999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  7, Batch 10000] loss: 0.962\n",
      "[Epoch  7, Batch 12000] loss: 0.958\n",
      "Epoch 7 completed in 27.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 84999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 86999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  8, Batch  2000] loss: 0.867\n",
      "[Epoch  8, Batch  4000] loss: 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 89499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 91499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  8, Batch  6000] loss: 0.887\n",
      "[Epoch  8, Batch  8000] loss: 0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 93499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 95499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  8, Batch 10000] loss: 0.917\n",
      "[Epoch  8, Batch 12000] loss: 0.908\n",
      "Epoch 8 completed in 26.17 seconds\n",
      "[Epoch  9, Batch  2000] loss: 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 97499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 99499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 101999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  9, Batch  4000] loss: 0.845\n",
      "[Epoch  9, Batch  6000] loss: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 103999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  9, Batch  8000] loss: 0.884\n",
      "[Epoch  9, Batch 10000] loss: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 107999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 109999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  9, Batch 12000] loss: 0.893\n",
      "Epoch 9 completed in 30.46 seconds\n",
      "[Epoch 10, Batch  2000] loss: 0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 111999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 114499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, Batch  4000] loss: 0.804\n",
      "[Epoch 10, Batch  6000] loss: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 116499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 118499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, Batch  8000] loss: 0.845\n",
      "[Epoch 10, Batch 10000] loss: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, Batch 12000] loss: 0.862\n",
      "Epoch 10 completed in 27.77 seconds\n",
      "[Epoch 11, Batch  2000] loss: 0.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 124499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 126999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11, Batch  4000] loss: 0.791\n",
      "[Epoch 11, Batch  6000] loss: 0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 128999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11, Batch  8000] loss: 0.804\n",
      "[Epoch 11, Batch 10000] loss: 0.821\n",
      "[Epoch 11, Batch 12000] loss: 0.843\n",
      "Epoch 11 completed in 26.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 132999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 134999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 136999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch  2000] loss: 0.722\n",
      "[Epoch 12, Batch  4000] loss: 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 139499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 141499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch  6000] loss: 0.782\n",
      "[Epoch 12, Batch  8000] loss: 0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 143499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 145499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch 10000] loss: 0.811\n",
      "[Epoch 12, Batch 12000] loss: 0.807\n",
      "Epoch 12 completed in 30.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 147499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 149499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch  2000] loss: 0.705\n",
      "[Epoch 13, Batch  4000] loss: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 151999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 153999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch  6000] loss: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 155999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch  8000] loss: 0.799\n",
      "[Epoch 13, Batch 10000] loss: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 159999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch 12000] loss: 0.777\n",
      "Epoch 13 completed in 35.91 seconds\n",
      "[Epoch 14, Batch  2000] loss: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 161999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 164499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch  4000] loss: 0.710\n",
      "[Epoch 14, Batch  6000] loss: 0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 166499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 168499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch  8000] loss: 0.742\n",
      "[Epoch 14, Batch 10000] loss: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 172499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch 12000] loss: 0.779\n",
      "Epoch 14 completed in 28.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 174499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 176999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15, Batch  2000] loss: 0.665\n",
      "[Epoch 15, Batch  4000] loss: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 178999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15, Batch  6000] loss: 0.702\n",
      "[Epoch 15, Batch  8000] loss: 0.730\n",
      "[Epoch 15, Batch 10000] loss: 0.748\n",
      "[Epoch 15, Batch 12000] loss: 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 182999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 184999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 186999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed in 26.35 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 189499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch  4000] loss: 0.673\n",
      "[Epoch 16, Batch  6000] loss: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 191499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 193499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch  8000] loss: 0.718\n",
      "[Epoch 16, Batch 10000] loss: 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 195499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 197499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch 12000] loss: 0.758\n",
      "Epoch 16 completed in 31.10 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 199499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 201999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17, Batch  4000] loss: 0.671\n",
      "[Epoch 17, Batch  6000] loss: 0.692\n",
      "[Epoch 17, Batch  8000] loss: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 203999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 205999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 207999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17, Batch 10000] loss: 0.714\n",
      "[Epoch 17, Batch 12000] loss: 0.721\n",
      "Epoch 17 completed in 23.22 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 209999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 211999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 214499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18, Batch  4000] loss: 0.637\n",
      "[Epoch 18, Batch  6000] loss: 0.678\n",
      "[Epoch 18, Batch  8000] loss: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 216499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 218499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 220499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18, Batch 10000] loss: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 222499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18, Batch 12000] loss: 0.720\n",
      "Epoch 18 completed in 27.80 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 224499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 226999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19, Batch  4000] loss: 0.646\n",
      "[Epoch 19, Batch  6000] loss: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 228999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 230999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19, Batch  8000] loss: 0.666\n",
      "[Epoch 19, Batch 10000] loss: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 232999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 234999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19, Batch 12000] loss: 0.691\n",
      "Epoch 19 completed in 32.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 236999 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch  2000] loss: 0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 239499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch  4000] loss: 0.630\n",
      "[Epoch 20, Batch  6000] loss: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 241499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 243499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch  8000] loss: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 245499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch 10000] loss: 0.682\n",
      "[Epoch 20, Batch 12000] loss: 0.713\n",
      "Epoch 20 completed in 41.53 seconds\n",
      "Finished Training CNN_LeakyReLU\n",
      "Model CNN_LeakyReLU saved to ./data/cifar_CNN_LeakyReLU.pth\n",
      "\n",
      "Evaluating CNN_LeakyReLU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 247499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 249499 that is less than the current step 249510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Performance Summary for CNN_LeakyReLU\n",
      "============================================================\n",
      "Total Training Time: 636.52 seconds\n",
      "Average Epoch Time: 31.83 seconds\n",
      "Overall Test Accuracy: 58.14%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  61.1%\n",
      "  car       :  72.8%\n",
      "  bird      :  50.9%\n",
      "  cat       :  46.6%\n",
      "  deer      :  57.3%\n",
      "  dog       :  37.7%\n",
      "  frog      :  58.1%\n",
      "  horse     :  50.5%\n",
      "  ship      :  81.6%\n",
      "  truck     :  64.8%\n",
      "============================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Processing Model: CNN_Tanh\n",
      "################################################################################\n",
      "\n",
      "Training CNN_Tanh...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1, Batch  4000] loss: 1.710\n",
      "[Epoch  1, Batch  6000] loss: 1.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1, Batch  8000] loss: 1.502\n",
      "[Epoch  1, Batch 10000] loss: 1.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1, Batch 12000] loss: 1.384\n",
      "Epoch 1 completed in 33.58 seconds\n",
      "[Epoch  2, Batch  2000] loss: 1.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 14499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2, Batch  4000] loss: 1.304\n",
      "[Epoch  2, Batch  6000] loss: 1.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 16499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 18499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2, Batch  8000] loss: 1.284\n",
      "[Epoch  2, Batch 10000] loss: 1.263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 22499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2, Batch 12000] loss: 1.229\n",
      "Epoch 2 completed in 28.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 24499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch  2000] loss: 1.196\n",
      "[Epoch  3, Batch  4000] loss: 1.160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 26999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 28999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch  6000] loss: 1.173\n",
      "[Epoch  3, Batch  8000] loss: 1.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 32999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch 10000] loss: 1.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 34999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  3, Batch 12000] loss: 1.161\n",
      "Epoch 3 completed in 39.40 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 36999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 39499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch  4000] loss: 1.090\n",
      "[Epoch  4, Batch  6000] loss: 1.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 41499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 43499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch  8000] loss: 1.082\n",
      "[Epoch  4, Batch 10000] loss: 1.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 45499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 47499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  4, Batch 12000] loss: 1.107\n",
      "Epoch 4 completed in 31.02 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 49499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 51999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch  4000] loss: 1.034\n",
      "[Epoch  5, Batch  6000] loss: 1.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 53999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 55999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch  8000] loss: 1.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 57999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  5, Batch 10000] loss: 1.037\n",
      "[Epoch  5, Batch 12000] loss: 1.017\n",
      "Epoch 5 completed in 32.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 59999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 61999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch  2000] loss: 0.986\n",
      "[Epoch  6, Batch  4000] loss: 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 64499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 66499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch  6000] loss: 0.983\n",
      "[Epoch  6, Batch  8000] loss: 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 68499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  6, Batch 10000] loss: 1.009\n",
      "[Epoch  6, Batch 12000] loss: 1.016\n",
      "Epoch 6 completed in 32.88 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 72499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 74499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  7, Batch  2000] loss: 0.925\n",
      "[Epoch  7, Batch  4000] loss: 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 76999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 78999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  7, Batch  6000] loss: 0.946\n",
      "[Epoch  7, Batch  8000] loss: 0.966\n",
      "[Epoch  7, Batch 10000] loss: 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 82999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 84999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  7, Batch 12000] loss: 0.957\n",
      "Epoch 7 completed in 26.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 86999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  8, Batch  2000] loss: 0.889\n",
      "[Epoch  8, Batch  4000] loss: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 89499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 91499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  8, Batch  6000] loss: 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 93499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  8, Batch  8000] loss: 0.926\n",
      "[Epoch  8, Batch 10000] loss: 0.891\n",
      "[Epoch  8, Batch 12000] loss: 0.940\n",
      "Epoch 8 completed in 33.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 95499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 97499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 99499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  9, Batch  2000] loss: 0.824\n",
      "[Epoch  9, Batch  4000] loss: 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 101999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 103999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  9, Batch  6000] loss: 0.881\n",
      "[Epoch  9, Batch  8000] loss: 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 107999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  9, Batch 10000] loss: 0.889\n",
      "[Epoch  9, Batch 12000] loss: 0.894\n",
      "Epoch 9 completed in 26.08 seconds\n",
      "[Epoch 10, Batch  2000] loss: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 109999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 111999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 114499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, Batch  4000] loss: 0.830\n",
      "[Epoch 10, Batch  6000] loss: 0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 116499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 118499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10, Batch  8000] loss: 0.850\n",
      "[Epoch 10, Batch 10000] loss: 0.884\n",
      "[Epoch 10, Batch 12000] loss: 0.855\n",
      "Epoch 10 completed in 23.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 124499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11, Batch  2000] loss: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 126999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11, Batch  4000] loss: 0.794\n",
      "[Epoch 11, Batch  6000] loss: 0.804\n",
      "[Epoch 11, Batch  8000] loss: 0.824\n",
      "[Epoch 11, Batch 10000] loss: 0.840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 128999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 132999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 134999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11, Batch 12000] loss: 0.866\n",
      "Epoch 11 completed in 27.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 136999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch  2000] loss: 0.727\n",
      "[Epoch 12, Batch  4000] loss: 0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 139499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 141499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch  6000] loss: 0.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 143499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 145499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12, Batch  8000] loss: 0.789\n",
      "[Epoch 12, Batch 10000] loss: 0.803\n",
      "[Epoch 12, Batch 12000] loss: 0.816\n",
      "Epoch 12 completed in 31.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 147499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 149499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch  2000] loss: 0.684\n",
      "[Epoch 13, Batch  4000] loss: 0.741\n",
      "[Epoch 13, Batch  6000] loss: 0.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 151999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 153999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 155999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch  8000] loss: 0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13, Batch 10000] loss: 0.783\n",
      "[Epoch 13, Batch 12000] loss: 0.801\n",
      "Epoch 13 completed in 28.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 159999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 161999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch  2000] loss: 0.661\n",
      "[Epoch 14, Batch  4000] loss: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 164499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 166499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch  6000] loss: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 168499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch  8000] loss: 0.745\n",
      "[Epoch 14, Batch 10000] loss: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 172499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14, Batch 12000] loss: 0.777\n",
      "Epoch 14 completed in 35.09 seconds\n",
      "[Epoch 15, Batch  2000] loss: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 174499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 176999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15, Batch  4000] loss: 0.699\n",
      "[Epoch 15, Batch  6000] loss: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 178999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15, Batch  8000] loss: 0.738\n",
      "[Epoch 15, Batch 10000] loss: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 182999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 184999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15, Batch 12000] loss: 0.740\n",
      "Epoch 15 completed in 29.89 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.618\n",
      "[Epoch 16, Batch  4000] loss: 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 186999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 189499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 191499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch  6000] loss: 0.673\n",
      "[Epoch 16, Batch  8000] loss: 0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 193499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 195499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch 10000] loss: 0.713\n",
      "[Epoch 16, Batch 12000] loss: 0.736\n",
      "Epoch 16 completed in 23.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 197499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 199499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17, Batch  2000] loss: 0.613\n",
      "[Epoch 17, Batch  4000] loss: 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 201999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 203999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17, Batch  6000] loss: 0.674\n",
      "[Epoch 17, Batch  8000] loss: 0.673\n",
      "[Epoch 17, Batch 10000] loss: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 205999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 207999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 209999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17, Batch 12000] loss: 0.723\n",
      "Epoch 17 completed in 27.38 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 211999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 214499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18, Batch  4000] loss: 0.636\n",
      "[Epoch 18, Batch  6000] loss: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 216499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 218499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 220499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18, Batch  8000] loss: 0.669\n",
      "[Epoch 18, Batch 10000] loss: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 222499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18, Batch 12000] loss: 0.674\n",
      "Epoch 18 completed in 28.22 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.546\n",
      "[Epoch 19, Batch  4000] loss: 0.613\n",
      "[Epoch 19, Batch  6000] loss: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 224499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 226999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 228999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 230999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19, Batch  8000] loss: 0.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 232999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19, Batch 10000] loss: 0.687\n",
      "[Epoch 19, Batch 12000] loss: 0.695\n",
      "Epoch 19 completed in 26.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 234999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 236999 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch  2000] loss: 0.554\n",
      "[Epoch 20, Batch  4000] loss: 0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 239499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 241499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch  6000] loss: 0.616\n",
      "[Epoch 20, Batch  8000] loss: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 243499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 245499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20, Batch 10000] loss: 0.658\n",
      "[Epoch 20, Batch 12000] loss: 0.668\n",
      "Epoch 20 completed in 28.39 seconds\n",
      "Finished Training CNN_Tanh\n",
      "Model CNN_Tanh saved to ./data/cifar_CNN_Tanh.pth\n",
      "\n",
      "Evaluating CNN_Tanh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 247499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 249499 that is less than the current step 249521. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Performance Summary for CNN_Tanh\n",
      "============================================================\n",
      "Total Training Time: 595.15 seconds\n",
      "Average Epoch Time: 29.76 seconds\n",
      "Overall Test Accuracy: 59.76%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  69.4%\n",
      "  car       :  70.5%\n",
      "  bird      :  47.5%\n",
      "  cat       :  27.1%\n",
      "  deer      :  44.1%\n",
      "  dog       :  58.1%\n",
      "  frog      :  69.6%\n",
      "  horse     :  66.1%\n",
      "  ship      :  80.5%\n",
      "  truck     :  64.7%\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION FUNCTION COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Model                     Test Accuracy        Training Time (s)   \n",
      "--------------------------------------------------------------------------------\n",
      "OriginalNet                61.07%                  583.15\n",
      "CNN_LeakyReLU              58.14%                  636.52\n",
      "CNN_Tanh                   59.76%                  595.15\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CNN_LeakyReLU/test_accuracy_bird</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_car</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_cat</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_deer</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_dog</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_frog</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_horse</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_plane</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_ship</td><td>▁</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_truck</td><td>▁</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CNN_LeakyReLU/test_accuracy_bird</td><td>50.9</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_car</td><td>72.8</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_cat</td><td>46.6</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_deer</td><td>57.3</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_dog</td><td>37.7</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_frog</td><td>58.1</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_horse</td><td>50.5</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_plane</td><td>61.1</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_ship</td><td>81.6</td></tr><tr><td>CNN_LeakyReLU/test_accuracy_truck</td><td>64.8</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-aardvark-1</strong> at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations/runs/cvjzl03c' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations/runs/cvjzl03c</a><br> View project at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Activations</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251031_204647-cvjzl03c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_data(config: TrainingConfig) -> Tuple[torch.utils.data.DataLoader,\n",
    "                                                    torch.utils.data.DataLoader,\n",
    "                                                    Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Prepare CIFAR-10 data loaders and class names.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (trainloader, testloader, classes)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "def main_activation_comparison():\n",
    "    \"\"\"\n",
    "    Main function to compare different activation functions.\n",
    "    \n",
    "    This function trains and evaluates three CNN models with different\n",
    "    activation functions:\n",
    "    - ReLU (baseline from tutorial)\n",
    "    - LeakyReLU (with negative slope of 0.01)\n",
    "    - Tanh\n",
    "    \n",
    "    All models have identical architecture except for the activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = TrainingConfig(\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        epochs=20,\n",
    "        batch_size=4,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"STATS413_HW3_Activations\",\n",
    "        entity=\"ohsono-ucla\",\n",
    "        config={\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"momentum\": config.momentum,\n",
    "            \"epochs\": config.epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"experiment\": \"activation_comparison\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check device\n",
    "    device = torch.device(config.device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    trainloader, testloader, classes = prepare_data(config)\n",
    "    print(f\"Data loaded successfully. Training samples: {len(trainloader.dataset)}, \"\n",
    "          f\"Test samples: {len(testloader.dataset)}\")\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize models with different activation functions\n",
    "    models = [\n",
    "        OriginalNet(),      # ReLU\n",
    "        LeakyReLUNet(),     # LeakyReLU\n",
    "        TanhNet()           # Tanh\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT: Comparing Different Activation Functions\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nModels to compare:\")\n",
    "    print(\"1. OriginalNet_ReLU: Baseline with ReLU activation\")\n",
    "    print(\"2. CNN_LeakyReLU: LeakyReLU with negative_slope=0.01\")\n",
    "    print(\"3. CNN_Tanh: Tanh activation\")\n",
    "    print(\"\\nAll models use the same architecture (conv-pool-conv-pool-fc-fc-fc)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = []\n",
    "    for model in models:\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"Processing Model: {model.model_name}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "\n",
    "        # Train\n",
    "        model.train_model(trainloader, config, criterion)\n",
    "\n",
    "        # Save model\n",
    "        save_path = f'./data/cifar_{model.model_name}.pth'\n",
    "        model.save_model(save_path)\n",
    "\n",
    "        # Evaluate\n",
    "        model.evaluate_model(testloader, classes, config)\n",
    "        \n",
    "        # Store results for comparison\n",
    "        results.append({\n",
    "            'name': model.model_name,\n",
    "            'accuracy': model.metrics.test_accuracy,\n",
    "            'training_time': model.metrics.total_training_time\n",
    "        })\n",
    "\n",
    "    # Print comparison summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ACTIVATION FUNCTION COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Model':<25} {'Test Accuracy':<20} {'Training Time (s)':<20}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    for result in results:\n",
    "        print(f\"{result['name']:<25} {result['accuracy']:>6.2f}%{'':<13} {result['training_time']:>10.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Close wandb\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main_activation_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ohsono/jupyterlab/wandb/run-20251031_211719-w4eyvpcg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet/runs/w4eyvpcg' target=\"_blank\">worthy-oath-1</a></strong> to <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet/runs/w4eyvpcg' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet/runs/w4eyvpcg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Preparing data...\n",
      "Data loaded successfully. Training samples: 50000, Test samples: 10000\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: ResNet with Residual Connections (OPTIONAL EXTRA CREDIT)\n",
      "================================================================================\n",
      "\n",
      "Network Structure:\n",
      "  Input: 3x32x32 RGB images\n",
      "  Conv1: 3 -> 16 channels (3x3, stride=1, padding=1)\n",
      "  BatchNorm + ReLU\n",
      "\n",
      "  ResBlock1: 16 -> 32 channels (stride=2, output: 32x16x16)\n",
      "    - Conv 3x3, stride=2\n",
      "    - BatchNorm -> ReLU\n",
      "    - Conv 3x3, stride=1\n",
      "    - BatchNorm\n",
      "    - Skip: 1x1 conv to match dimensions\n",
      "    - Add skip + main path\n",
      "    - ReLU\n",
      "\n",
      "  ResBlock2: 32 -> 64 channels (stride=2, output: 64x8x8)\n",
      "    - Same structure as ResBlock1\n",
      "\n",
      "  ResBlock3: 64 -> 64 channels (stride=1, output: 64x8x8)\n",
      "    - Same structure, no dimension change\n",
      "\n",
      "  Global Average Pooling: 64x8x8 -> 64x1x1\n",
      "  FC: 64 -> 10 classes\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Processing Model: ResNet_Custom\n",
      "################################################################################\n",
      "\n",
      "Training ResNet_Custom...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 1.914\n",
      "[Epoch  1, Batch  4000] loss: 1.715\n",
      "[Epoch  1, Batch  6000] loss: 1.590\n",
      "[Epoch  1, Batch  8000] loss: 1.486\n",
      "[Epoch  1, Batch 10000] loss: 1.411\n",
      "[Epoch  1, Batch 12000] loss: 1.335\n",
      "Epoch 1 completed in 33.93 seconds\n",
      "[Epoch  2, Batch  2000] loss: 1.249\n",
      "[Epoch  2, Batch  4000] loss: 1.210\n",
      "[Epoch  2, Batch  6000] loss: 1.159\n",
      "[Epoch  2, Batch  8000] loss: 1.140\n",
      "[Epoch  2, Batch 10000] loss: 1.115\n",
      "[Epoch  2, Batch 12000] loss: 1.068\n",
      "Epoch 2 completed in 28.55 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.018\n",
      "[Epoch  3, Batch  4000] loss: 0.994\n",
      "[Epoch  3, Batch  6000] loss: 0.973\n",
      "[Epoch  3, Batch  8000] loss: 0.969\n",
      "[Epoch  3, Batch 10000] loss: 0.940\n",
      "[Epoch  3, Batch 12000] loss: 0.935\n",
      "Epoch 3 completed in 28.19 seconds\n",
      "[Epoch  4, Batch  2000] loss: 0.867\n",
      "[Epoch  4, Batch  4000] loss: 0.863\n",
      "[Epoch  4, Batch  6000] loss: 0.852\n",
      "[Epoch  4, Batch  8000] loss: 0.849\n",
      "[Epoch  4, Batch 10000] loss: 0.847\n",
      "[Epoch  4, Batch 12000] loss: 0.824\n",
      "Epoch 4 completed in 27.69 seconds\n",
      "[Epoch  5, Batch  2000] loss: 0.766\n",
      "[Epoch  5, Batch  4000] loss: 0.773\n",
      "[Epoch  5, Batch  6000] loss: 0.777\n",
      "[Epoch  5, Batch  8000] loss: 0.763\n",
      "[Epoch  5, Batch 10000] loss: 0.768\n",
      "[Epoch  5, Batch 12000] loss: 0.735\n",
      "Epoch 5 completed in 46.99 seconds\n",
      "[Epoch  6, Batch  2000] loss: 0.680\n",
      "[Epoch  6, Batch  4000] loss: 0.683\n",
      "[Epoch  6, Batch  6000] loss: 0.690\n",
      "[Epoch  6, Batch  8000] loss: 0.707\n",
      "[Epoch  6, Batch 10000] loss: 0.707\n",
      "[Epoch  6, Batch 12000] loss: 0.687\n",
      "Epoch 6 completed in 46.22 seconds\n",
      "[Epoch  7, Batch  2000] loss: 0.625\n",
      "[Epoch  7, Batch  4000] loss: 0.632\n",
      "[Epoch  7, Batch  6000] loss: 0.640\n",
      "[Epoch  7, Batch  8000] loss: 0.662\n",
      "[Epoch  7, Batch 10000] loss: 0.643\n",
      "[Epoch  7, Batch 12000] loss: 0.652\n",
      "Epoch 7 completed in 35.81 seconds\n",
      "[Epoch  8, Batch  2000] loss: 0.559\n",
      "[Epoch  8, Batch  4000] loss: 0.593\n",
      "[Epoch  8, Batch  6000] loss: 0.593\n",
      "[Epoch  8, Batch  8000] loss: 0.602\n",
      "[Epoch  8, Batch 10000] loss: 0.599\n",
      "[Epoch  8, Batch 12000] loss: 0.609\n",
      "Epoch 8 completed in 64.89 seconds\n",
      "[Epoch  9, Batch  2000] loss: 0.543\n",
      "[Epoch  9, Batch  4000] loss: 0.552\n",
      "[Epoch  9, Batch  6000] loss: 0.548\n",
      "[Epoch  9, Batch  8000] loss: 0.550\n",
      "[Epoch  9, Batch 10000] loss: 0.566\n",
      "[Epoch  9, Batch 12000] loss: 0.563\n",
      "Epoch 9 completed in 66.24 seconds\n",
      "[Epoch 10, Batch  2000] loss: 0.495\n",
      "[Epoch 10, Batch  4000] loss: 0.514\n",
      "[Epoch 10, Batch  6000] loss: 0.507\n",
      "[Epoch 10, Batch  8000] loss: 0.529\n",
      "[Epoch 10, Batch 10000] loss: 0.536\n",
      "[Epoch 10, Batch 12000] loss: 0.525\n",
      "Epoch 10 completed in 62.26 seconds\n",
      "[Epoch 11, Batch  2000] loss: 0.474\n",
      "[Epoch 11, Batch  4000] loss: 0.482\n",
      "[Epoch 11, Batch  6000] loss: 0.471\n",
      "[Epoch 11, Batch  8000] loss: 0.489\n",
      "[Epoch 11, Batch 10000] loss: 0.484\n",
      "[Epoch 11, Batch 12000] loss: 0.505\n",
      "Epoch 11 completed in 31.46 seconds\n",
      "[Epoch 12, Batch  2000] loss: 0.435\n",
      "[Epoch 12, Batch  4000] loss: 0.461\n",
      "[Epoch 12, Batch  6000] loss: 0.443\n",
      "[Epoch 12, Batch  8000] loss: 0.463\n",
      "[Epoch 12, Batch 10000] loss: 0.468\n",
      "[Epoch 12, Batch 12000] loss: 0.452\n",
      "Epoch 12 completed in 30.44 seconds\n",
      "[Epoch 13, Batch  2000] loss: 0.407\n",
      "[Epoch 13, Batch  4000] loss: 0.424\n",
      "[Epoch 13, Batch  6000] loss: 0.428\n",
      "[Epoch 13, Batch  8000] loss: 0.426\n",
      "[Epoch 13, Batch 10000] loss: 0.437\n",
      "[Epoch 13, Batch 12000] loss: 0.440\n",
      "Epoch 13 completed in 52.67 seconds\n",
      "[Epoch 14, Batch  2000] loss: 0.380\n",
      "[Epoch 14, Batch  4000] loss: 0.395\n",
      "[Epoch 14, Batch  6000] loss: 0.402\n",
      "[Epoch 14, Batch  8000] loss: 0.409\n",
      "[Epoch 14, Batch 10000] loss: 0.414\n",
      "[Epoch 14, Batch 12000] loss: 0.428\n",
      "Epoch 14 completed in 58.04 seconds\n",
      "[Epoch 15, Batch  2000] loss: 0.354\n",
      "[Epoch 15, Batch  4000] loss: 0.359\n",
      "[Epoch 15, Batch  6000] loss: 0.381\n",
      "[Epoch 15, Batch  8000] loss: 0.400\n",
      "[Epoch 15, Batch 10000] loss: 0.381\n",
      "[Epoch 15, Batch 12000] loss: 0.416\n",
      "Epoch 15 completed in 43.01 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.321\n",
      "[Epoch 16, Batch  4000] loss: 0.343\n",
      "[Epoch 16, Batch  6000] loss: 0.363\n",
      "[Epoch 16, Batch  8000] loss: 0.364\n",
      "[Epoch 16, Batch 10000] loss: 0.377\n",
      "[Epoch 16, Batch 12000] loss: 0.366\n",
      "Epoch 16 completed in 59.94 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.309\n",
      "[Epoch 17, Batch  4000] loss: 0.317\n",
      "[Epoch 17, Batch  6000] loss: 0.352\n",
      "[Epoch 17, Batch  8000] loss: 0.353\n",
      "[Epoch 17, Batch 10000] loss: 0.347\n",
      "[Epoch 17, Batch 12000] loss: 0.354\n",
      "Epoch 17 completed in 29.41 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.299\n",
      "[Epoch 18, Batch  4000] loss: 0.313\n",
      "[Epoch 18, Batch  6000] loss: 0.300\n",
      "[Epoch 18, Batch  8000] loss: 0.324\n",
      "[Epoch 18, Batch 10000] loss: 0.340\n",
      "[Epoch 18, Batch 12000] loss: 0.345\n",
      "Epoch 18 completed in 57.40 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.288\n",
      "[Epoch 19, Batch  4000] loss: 0.288\n",
      "[Epoch 19, Batch  6000] loss: 0.303\n",
      "[Epoch 19, Batch  8000] loss: 0.311\n",
      "[Epoch 19, Batch 10000] loss: 0.310\n",
      "[Epoch 19, Batch 12000] loss: 0.306\n",
      "Epoch 19 completed in 48.80 seconds\n",
      "[Epoch 20, Batch  2000] loss: 0.266\n",
      "[Epoch 20, Batch  4000] loss: 0.279\n",
      "[Epoch 20, Batch  6000] loss: 0.279\n",
      "[Epoch 20, Batch  8000] loss: 0.288\n",
      "[Epoch 20, Batch 10000] loss: 0.295\n",
      "[Epoch 20, Batch 12000] loss: 0.312\n",
      "Epoch 20 completed in 43.42 seconds\n",
      "Finished Training ResNet_Custom\n",
      "Model ResNet_Custom saved to ./data/cifar_ResNet_Custom.pth\n",
      "\n",
      "Evaluating ResNet_Custom...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for ResNet_Custom\n",
      "============================================================\n",
      "Total Training Time: 895.37 seconds\n",
      "Average Epoch Time: 44.77 seconds\n",
      "Overall Test Accuracy: 80.58%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  89.8%\n",
      "  car       :  88.6%\n",
      "  bird      :  67.9%\n",
      "  cat       :  56.3%\n",
      "  deer      :  85.2%\n",
      "  dog       :  77.8%\n",
      "  frog      :  86.0%\n",
      "  horse     :  80.2%\n",
      "  ship      :  89.6%\n",
      "  truck     :  84.4%\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESNET EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Model: ResNet_Custom\n",
      "Test Accuracy: 80.58%\n",
      "Training Time: 895.37 seconds\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ResNet_Custom/test_accuracy_bird</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_car</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_cat</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_deer</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_dog</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_frog</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_horse</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_plane</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_ship</td><td>▁</td></tr><tr><td>ResNet_Custom/test_accuracy_truck</td><td>▁</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ResNet_Custom/test_accuracy_bird</td><td>67.9</td></tr><tr><td>ResNet_Custom/test_accuracy_car</td><td>88.6</td></tr><tr><td>ResNet_Custom/test_accuracy_cat</td><td>56.3</td></tr><tr><td>ResNet_Custom/test_accuracy_deer</td><td>85.2</td></tr><tr><td>ResNet_Custom/test_accuracy_dog</td><td>77.8</td></tr><tr><td>ResNet_Custom/test_accuracy_frog</td><td>86</td></tr><tr><td>ResNet_Custom/test_accuracy_horse</td><td>80.2</td></tr><tr><td>ResNet_Custom/test_accuracy_plane</td><td>89.8</td></tr><tr><td>ResNet_Custom/test_accuracy_ship</td><td>89.6</td></tr><tr><td>ResNet_Custom/test_accuracy_truck</td><td>84.4</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-oath-1</strong> at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet/runs/w4eyvpcg' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet/runs/w4eyvpcg</a><br> View project at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_ResNet</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251031_211719-w4eyvpcg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def main_resnet():\n",
    "    \"\"\"\n",
    "    Main function to train ResNet with residual connections (OPTIONAL).\n",
    "    \n",
    "    This function demonstrates the use of residual connections, which allow\n",
    "    for training deeper networks by providing skip connections that help\n",
    "    gradient flow during backpropagation.\n",
    "    \n",
    "    Network Architecture:\n",
    "    - Initial 3x3 conv: 3 channels -> 16 channels\n",
    "    - ResBlock 1: 16 -> 32 channels, stride=2 (downsample to 16x16)\n",
    "    - ResBlock 2: 32 -> 64 channels, stride=2 (downsample to 8x8)\n",
    "    - ResBlock 3: 64 -> 64 channels, stride=1 (maintain 8x8)\n",
    "    - Global Average Pooling\n",
    "    - FC layer: 64 -> 10 classes\n",
    "    \n",
    "    Each residual block contains:\n",
    "    - Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm\n",
    "    - Skip connection (1x1 conv if dimensions change)\n",
    "    - Addition of skip and main path\n",
    "    - Final ReLU\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = TrainingConfig(\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        epochs=20,\n",
    "        batch_size=4,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"STATS413_HW3_ResNet\",\n",
    "        entity=\"ohsono-ucla\",\n",
    "        config={\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"momentum\": config.momentum,\n",
    "            \"epochs\": config.epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"experiment\": \"resnet_residual_connections\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check device\n",
    "    device = torch.device(config.device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    trainloader, testloader, classes = prepare_data(config)\n",
    "    print(f\"Data loaded successfully. Training samples: {len(trainloader.dataset)}, \"\n",
    "          f\"Test samples: {len(testloader.dataset)}\")\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize ResNet model\n",
    "    model = ResNet()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT: ResNet with Residual Connections (OPTIONAL EXTRA CREDIT)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNetwork Structure:\")\n",
    "    print(\"  Input: 3x32x32 RGB images\")\n",
    "    print(\"  Conv1: 3 -> 16 channels (3x3, stride=1, padding=1)\")\n",
    "    print(\"  BatchNorm + ReLU\")\n",
    "    print(\"\\n  ResBlock1: 16 -> 32 channels (stride=2, output: 32x16x16)\")\n",
    "    print(\"    - Conv 3x3, stride=2\")\n",
    "    print(\"    - BatchNorm -> ReLU\")\n",
    "    print(\"    - Conv 3x3, stride=1\")\n",
    "    print(\"    - BatchNorm\")\n",
    "    print(\"    - Skip: 1x1 conv to match dimensions\")\n",
    "    print(\"    - Add skip + main path\")\n",
    "    print(\"    - ReLU\")\n",
    "    print(\"\\n  ResBlock2: 32 -> 64 channels (stride=2, output: 64x8x8)\")\n",
    "    print(\"    - Same structure as ResBlock1\")\n",
    "    print(\"\\n  ResBlock3: 64 -> 64 channels (stride=1, output: 64x8x8)\")\n",
    "    print(\"    - Same structure, no dimension change\")\n",
    "    print(\"\\n  Global Average Pooling: 64x8x8 -> 64x1x1\")\n",
    "    print(\"  FC: 64 -> 10 classes\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"Processing Model: {model.model_name}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "\n",
    "    # Train\n",
    "    model.train_model(trainloader, config, criterion)\n",
    "\n",
    "    # Save model\n",
    "    save_path = f'./data/cifar_{model.model_name}.pth'\n",
    "    model.save_model(save_path)\n",
    "\n",
    "    # Evaluate\n",
    "    model.evaluate_model(testloader, classes, config)\n",
    "\n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RESNET EXPERIMENT SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Model: {model.model_name}\")\n",
    "    print(f\"Test Accuracy: {model.metrics.test_accuracy:.2f}%\")\n",
    "    print(f\"Training Time: {model.metrics.total_training_time:.2f} seconds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Close wandb\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main_resnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER COMPARISON EXPERIMENTS\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## EXPERIMENT 1: LEARNING RATE COMPARISON\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Tracking run with wandb version 0.22.3\n",
    "Run data is saved locally in /home/ohsono/jupyterlab/wandb/run-20251031_215633-ue9jy7cy\n",
    "Syncing run learning_rate_comparison to Weights & Biases (docs)\n",
    "View project at https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters\n",
    "View run at https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/ue9jy7cy\n",
    "\n",
    "Comparing Learning Rates:\n",
    "  - Baseline: 0.001 (tutorial default)\n",
    "  - Smaller:  0.0001 (10x smaller)\n",
    "  - Larger:   0.01 (10x larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER COMPARISON EXPERIMENTS\n",
      "================================================================================\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "EXPERIMENT 1: LEARNING RATE COMPARISON\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ohsono/jupyterlab/wandb/run-20251031_215633-ue9jy7cy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/ue9jy7cy' target=\"_blank\">learning_rate_comparison</a></strong> to <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/ue9jy7cy' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/ue9jy7cy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Learning Rates:\n",
      "  - Baseline: 0.001 (tutorial default)\n",
      "  - Smaller:  0.0001 (10x smaller)\n",
      "  - Larger:   0.01 (10x larger)\n",
      "\n",
      "################################################################################\n",
      "Training: OriginalNet\n",
      "################################################################################\n",
      "\n",
      "Training OriginalNet...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.175\n",
      "[Epoch  1, Batch  4000] loss: 1.906\n",
      "[Epoch  1, Batch  6000] loss: 1.714\n",
      "[Epoch  1, Batch  8000] loss: 1.593\n",
      "[Epoch  1, Batch 10000] loss: 1.527\n",
      "[Epoch  1, Batch 12000] loss: 1.487\n",
      "Epoch 1 completed in 37.42 seconds\n",
      "[Epoch  2, Batch  2000] loss: 1.427\n",
      "[Epoch  2, Batch  4000] loss: 1.409\n",
      "[Epoch  2, Batch  6000] loss: 1.389\n",
      "[Epoch  2, Batch  8000] loss: 1.362\n",
      "[Epoch  2, Batch 10000] loss: 1.309\n",
      "[Epoch  2, Batch 12000] loss: 1.298\n",
      "Epoch 2 completed in 31.50 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.228\n",
      "[Epoch  3, Batch  4000] loss: 1.219\n",
      "[Epoch  3, Batch  6000] loss: 1.234\n",
      "[Epoch  3, Batch  8000] loss: 1.204\n",
      "[Epoch  3, Batch 10000] loss: 1.192\n",
      "[Epoch  3, Batch 12000] loss: 1.196\n",
      "Epoch 3 completed in 26.75 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.115\n",
      "[Epoch  4, Batch  4000] loss: 1.115\n",
      "[Epoch  4, Batch  6000] loss: 1.117\n",
      "[Epoch  4, Batch  8000] loss: 1.123\n",
      "[Epoch  4, Batch 10000] loss: 1.119\n",
      "[Epoch  4, Batch 12000] loss: 1.106\n",
      "Epoch 4 completed in 37.99 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.042\n",
      "[Epoch  5, Batch  4000] loss: 1.039\n",
      "[Epoch  5, Batch  6000] loss: 1.051\n",
      "[Epoch  5, Batch  8000] loss: 1.051\n",
      "[Epoch  5, Batch 10000] loss: 1.071\n",
      "[Epoch  5, Batch 12000] loss: 1.047\n",
      "Epoch 5 completed in 32.35 seconds\n",
      "[Epoch  6, Batch  2000] loss: 0.984\n",
      "[Epoch  6, Batch  4000] loss: 1.005\n",
      "[Epoch  6, Batch  6000] loss: 0.994\n",
      "[Epoch  6, Batch  8000] loss: 0.995\n",
      "[Epoch  6, Batch 10000] loss: 0.995\n",
      "[Epoch  6, Batch 12000] loss: 1.007\n",
      "Epoch 6 completed in 36.57 seconds\n",
      "[Epoch  7, Batch  2000] loss: 0.934\n",
      "[Epoch  7, Batch  4000] loss: 0.929\n",
      "[Epoch  7, Batch  6000] loss: 0.947\n",
      "[Epoch  7, Batch  8000] loss: 0.956\n",
      "[Epoch  7, Batch 10000] loss: 0.978\n",
      "[Epoch  7, Batch 12000] loss: 0.973\n",
      "Epoch 7 completed in 33.92 seconds\n",
      "[Epoch  8, Batch  2000] loss: 0.864\n",
      "[Epoch  8, Batch  4000] loss: 0.906\n",
      "[Epoch  8, Batch  6000] loss: 0.903\n",
      "[Epoch  8, Batch  8000] loss: 0.929\n",
      "[Epoch  8, Batch 10000] loss: 0.941\n",
      "[Epoch  8, Batch 12000] loss: 0.937\n",
      "Epoch 8 completed in 28.82 seconds\n",
      "[Epoch  9, Batch  2000] loss: 0.829\n",
      "[Epoch  9, Batch  4000] loss: 0.861\n",
      "[Epoch  9, Batch  6000] loss: 0.870\n",
      "[Epoch  9, Batch  8000] loss: 0.876\n",
      "[Epoch  9, Batch 10000] loss: 0.903\n",
      "[Epoch  9, Batch 12000] loss: 0.917\n",
      "Epoch 9 completed in 32.98 seconds\n",
      "[Epoch 10, Batch  2000] loss: 0.812\n",
      "[Epoch 10, Batch  4000] loss: 0.830\n",
      "[Epoch 10, Batch  6000] loss: 0.854\n",
      "[Epoch 10, Batch  8000] loss: 0.838\n",
      "[Epoch 10, Batch 10000] loss: 0.873\n",
      "[Epoch 10, Batch 12000] loss: 0.887\n",
      "Epoch 10 completed in 34.11 seconds\n",
      "[Epoch 11, Batch  2000] loss: 0.762\n",
      "[Epoch 11, Batch  4000] loss: 0.790\n",
      "[Epoch 11, Batch  6000] loss: 0.819\n",
      "[Epoch 11, Batch  8000] loss: 0.852\n",
      "[Epoch 11, Batch 10000] loss: 0.847\n",
      "[Epoch 11, Batch 12000] loss: 0.853\n",
      "Epoch 11 completed in 28.73 seconds\n",
      "[Epoch 12, Batch  2000] loss: 0.754\n",
      "[Epoch 12, Batch  4000] loss: 0.773\n",
      "[Epoch 12, Batch  6000] loss: 0.794\n",
      "[Epoch 12, Batch  8000] loss: 0.831\n",
      "[Epoch 12, Batch 10000] loss: 0.829\n",
      "[Epoch 12, Batch 12000] loss: 0.832\n",
      "Epoch 12 completed in 31.17 seconds\n",
      "[Epoch 13, Batch  2000] loss: 0.700\n",
      "[Epoch 13, Batch  4000] loss: 0.767\n",
      "[Epoch 13, Batch  6000] loss: 0.791\n",
      "[Epoch 13, Batch  8000] loss: 0.796\n",
      "[Epoch 13, Batch 10000] loss: 0.803\n",
      "[Epoch 13, Batch 12000] loss: 0.811\n",
      "Epoch 13 completed in 34.37 seconds\n",
      "[Epoch 14, Batch  2000] loss: 0.680\n",
      "[Epoch 14, Batch  4000] loss: 0.738\n",
      "[Epoch 14, Batch  6000] loss: 0.763\n",
      "[Epoch 14, Batch  8000] loss: 0.771\n",
      "[Epoch 14, Batch 10000] loss: 0.775\n",
      "[Epoch 14, Batch 12000] loss: 0.829\n",
      "Epoch 14 completed in 24.68 seconds\n",
      "[Epoch 15, Batch  2000] loss: 0.676\n",
      "[Epoch 15, Batch  4000] loss: 0.723\n",
      "[Epoch 15, Batch  6000] loss: 0.737\n",
      "[Epoch 15, Batch  8000] loss: 0.746\n",
      "[Epoch 15, Batch 10000] loss: 0.785\n",
      "[Epoch 15, Batch 12000] loss: 0.772\n",
      "Epoch 15 completed in 20.35 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.662\n",
      "[Epoch 16, Batch  4000] loss: 0.688\n",
      "[Epoch 16, Batch  6000] loss: 0.722\n",
      "[Epoch 16, Batch  8000] loss: 0.740\n",
      "[Epoch 16, Batch 10000] loss: 0.742\n",
      "[Epoch 16, Batch 12000] loss: 0.772\n",
      "Epoch 16 completed in 28.98 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.658\n",
      "[Epoch 17, Batch  4000] loss: 0.683\n",
      "[Epoch 17, Batch  6000] loss: 0.706\n",
      "[Epoch 17, Batch  8000] loss: 0.732\n",
      "[Epoch 17, Batch 10000] loss: 0.749\n",
      "[Epoch 17, Batch 12000] loss: 0.768\n",
      "Epoch 17 completed in 28.90 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.646\n",
      "[Epoch 18, Batch  4000] loss: 0.675\n",
      "[Epoch 18, Batch  6000] loss: 0.698\n",
      "[Epoch 18, Batch  8000] loss: 0.714\n",
      "[Epoch 18, Batch 10000] loss: 0.732\n",
      "[Epoch 18, Batch 12000] loss: 0.755\n",
      "Epoch 18 completed in 34.89 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.618\n",
      "[Epoch 19, Batch  4000] loss: 0.688\n",
      "[Epoch 19, Batch  6000] loss: 0.686\n",
      "[Epoch 19, Batch  8000] loss: 0.691\n",
      "[Epoch 19, Batch 10000] loss: 0.727\n",
      "[Epoch 19, Batch 12000] loss: 0.720\n",
      "Epoch 19 completed in 26.43 seconds\n",
      "[Epoch 20, Batch  2000] loss: 0.617\n",
      "[Epoch 20, Batch  4000] loss: 0.644\n",
      "[Epoch 20, Batch  6000] loss: 0.679\n",
      "[Epoch 20, Batch  8000] loss: 0.700\n",
      "[Epoch 20, Batch 10000] loss: 0.730\n",
      "[Epoch 20, Batch 12000] loss: 0.728\n",
      "Epoch 20 completed in 29.31 seconds\n",
      "Finished Training OriginalNet\n",
      "Model OriginalNet saved to ./data/cifar_OriginalNet.pth\n",
      "\n",
      "Evaluating OriginalNet...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for OriginalNet\n",
      "============================================================\n",
      "Total Training Time: 620.22 seconds\n",
      "Average Epoch Time: 31.01 seconds\n",
      "Overall Test Accuracy: 60.63%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  66.7%\n",
      "  car       :  64.7%\n",
      "  bird      :  46.8%\n",
      "  cat       :  37.5%\n",
      "  deer      :  60.8%\n",
      "  dog       :  40.2%\n",
      "  frog      :  77.3%\n",
      "  horse     :  67.4%\n",
      "  ship      :  70.8%\n",
      "  truck     :  74.1%\n",
      "============================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Training: LR_0.0001\n",
      "################################################################################\n",
      "\n",
      "Training LR_0.0001...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.304\n",
      "[Epoch  1, Batch  4000] loss: 2.302\n",
      "[Epoch  1, Batch  6000] loss: 2.297\n",
      "[Epoch  1, Batch  8000] loss: 2.281\n",
      "[Epoch  1, Batch 10000] loss: 2.235\n",
      "[Epoch  1, Batch 12000] loss: 2.141\n",
      "Epoch 1 completed in 25.54 seconds\n",
      "[Epoch  2, Batch  2000] loss: 2.038\n",
      "[Epoch  2, Batch  4000] loss: 1.940\n",
      "[Epoch  2, Batch  6000] loss: 1.891\n",
      "[Epoch  2, Batch  8000] loss: 1.847\n",
      "[Epoch  2, Batch 10000] loss: 1.789\n",
      "[Epoch  2, Batch 12000] loss: 1.732\n",
      "Epoch 2 completed in 36.78 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.666\n",
      "[Epoch  3, Batch  4000] loss: 1.637\n",
      "[Epoch  3, Batch  6000] loss: 1.622\n",
      "[Epoch  3, Batch  8000] loss: 1.579\n",
      "[Epoch  3, Batch 10000] loss: 1.569\n",
      "[Epoch  3, Batch 12000] loss: 1.566\n",
      "Epoch 3 completed in 32.29 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.511\n",
      "[Epoch  4, Batch  4000] loss: 1.504\n",
      "[Epoch  4, Batch  6000] loss: 1.500\n",
      "[Epoch  4, Batch  8000] loss: 1.487\n",
      "[Epoch  4, Batch 10000] loss: 1.485\n",
      "[Epoch  4, Batch 12000] loss: 1.452\n",
      "Epoch 4 completed in 34.35 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.415\n",
      "[Epoch  5, Batch  4000] loss: 1.410\n",
      "[Epoch  5, Batch  6000] loss: 1.419\n",
      "[Epoch  5, Batch  8000] loss: 1.395\n",
      "[Epoch  5, Batch 10000] loss: 1.388\n",
      "[Epoch  5, Batch 12000] loss: 1.395\n",
      "Epoch 5 completed in 24.84 seconds\n",
      "[Epoch  6, Batch  2000] loss: 1.360\n",
      "[Epoch  6, Batch  4000] loss: 1.343\n",
      "[Epoch  6, Batch  6000] loss: 1.328\n",
      "[Epoch  6, Batch  8000] loss: 1.346\n",
      "[Epoch  6, Batch 10000] loss: 1.320\n",
      "[Epoch  6, Batch 12000] loss: 1.309\n",
      "Epoch 6 completed in 24.47 seconds\n",
      "[Epoch  7, Batch  2000] loss: 1.300\n",
      "[Epoch  7, Batch  4000] loss: 1.281\n",
      "[Epoch  7, Batch  6000] loss: 1.292\n",
      "[Epoch  7, Batch  8000] loss: 1.292\n",
      "[Epoch  7, Batch 10000] loss: 1.253\n",
      "[Epoch  7, Batch 12000] loss: 1.256\n",
      "Epoch 7 completed in 24.97 seconds\n",
      "[Epoch  8, Batch  2000] loss: 1.242\n",
      "[Epoch  8, Batch  4000] loss: 1.246\n",
      "[Epoch  8, Batch  6000] loss: 1.224\n",
      "[Epoch  8, Batch  8000] loss: 1.225\n",
      "[Epoch  8, Batch 10000] loss: 1.215\n",
      "[Epoch  8, Batch 12000] loss: 1.220\n",
      "Epoch 8 completed in 24.12 seconds\n",
      "[Epoch  9, Batch  2000] loss: 1.187\n",
      "[Epoch  9, Batch  4000] loss: 1.190\n",
      "[Epoch  9, Batch  6000] loss: 1.185\n",
      "[Epoch  9, Batch  8000] loss: 1.184\n",
      "[Epoch  9, Batch 10000] loss: 1.182\n",
      "[Epoch  9, Batch 12000] loss: 1.189\n",
      "Epoch 9 completed in 43.44 seconds\n",
      "[Epoch 10, Batch  2000] loss: 1.128\n",
      "[Epoch 10, Batch  4000] loss: 1.155\n",
      "[Epoch 10, Batch  6000] loss: 1.135\n",
      "[Epoch 10, Batch  8000] loss: 1.152\n",
      "[Epoch 10, Batch 10000] loss: 1.169\n",
      "[Epoch 10, Batch 12000] loss: 1.152\n",
      "Epoch 10 completed in 33.16 seconds\n",
      "[Epoch 11, Batch  2000] loss: 1.107\n",
      "[Epoch 11, Batch  4000] loss: 1.113\n",
      "[Epoch 11, Batch  6000] loss: 1.114\n",
      "[Epoch 11, Batch  8000] loss: 1.121\n",
      "[Epoch 11, Batch 10000] loss: 1.115\n",
      "[Epoch 11, Batch 12000] loss: 1.115\n",
      "Epoch 11 completed in 40.86 seconds\n",
      "[Epoch 12, Batch  2000] loss: 1.059\n",
      "[Epoch 12, Batch  4000] loss: 1.110\n",
      "[Epoch 12, Batch  6000] loss: 1.080\n",
      "[Epoch 12, Batch  8000] loss: 1.085\n",
      "[Epoch 12, Batch 10000] loss: 1.064\n",
      "[Epoch 12, Batch 12000] loss: 1.097\n",
      "Epoch 12 completed in 42.26 seconds\n",
      "[Epoch 13, Batch  2000] loss: 1.060\n",
      "[Epoch 13, Batch  4000] loss: 1.063\n",
      "[Epoch 13, Batch  6000] loss: 1.052\n",
      "[Epoch 13, Batch  8000] loss: 1.036\n",
      "[Epoch 13, Batch 10000] loss: 1.053\n",
      "[Epoch 13, Batch 12000] loss: 1.068\n",
      "Epoch 13 completed in 32.96 seconds\n",
      "[Epoch 14, Batch  2000] loss: 1.021\n",
      "[Epoch 14, Batch  4000] loss: 1.026\n",
      "[Epoch 14, Batch  6000] loss: 1.047\n",
      "[Epoch 14, Batch  8000] loss: 1.032\n",
      "[Epoch 14, Batch 10000] loss: 1.029\n",
      "[Epoch 14, Batch 12000] loss: 1.022\n",
      "Epoch 14 completed in 27.12 seconds\n",
      "[Epoch 15, Batch  2000] loss: 0.985\n",
      "[Epoch 15, Batch  4000] loss: 1.021\n",
      "[Epoch 15, Batch  6000] loss: 1.008\n",
      "[Epoch 15, Batch  8000] loss: 1.003\n",
      "[Epoch 15, Batch 10000] loss: 0.991\n",
      "[Epoch 15, Batch 12000] loss: 1.003\n",
      "Epoch 15 completed in 43.01 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.967\n",
      "[Epoch 16, Batch  4000] loss: 0.978\n",
      "[Epoch 16, Batch  6000] loss: 0.979\n",
      "[Epoch 16, Batch  8000] loss: 0.993\n",
      "[Epoch 16, Batch 10000] loss: 0.977\n",
      "[Epoch 16, Batch 12000] loss: 0.976\n",
      "Epoch 16 completed in 36.89 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.941\n",
      "[Epoch 17, Batch  4000] loss: 0.964\n",
      "[Epoch 17, Batch  6000] loss: 0.972\n",
      "[Epoch 17, Batch  8000] loss: 0.955\n",
      "[Epoch 17, Batch 10000] loss: 0.936\n",
      "[Epoch 17, Batch 12000] loss: 0.961\n",
      "Epoch 17 completed in 24.72 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.937\n",
      "[Epoch 18, Batch  4000] loss: 0.938\n",
      "[Epoch 18, Batch  6000] loss: 0.918\n",
      "[Epoch 18, Batch  8000] loss: 0.926\n",
      "[Epoch 18, Batch 10000] loss: 0.936\n",
      "[Epoch 18, Batch 12000] loss: 0.950\n",
      "Epoch 18 completed in 36.94 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.904\n",
      "[Epoch 19, Batch  4000] loss: 0.907\n",
      "[Epoch 19, Batch  6000] loss: 0.892\n",
      "[Epoch 19, Batch  8000] loss: 0.915\n",
      "[Epoch 19, Batch 10000] loss: 0.938\n",
      "[Epoch 19, Batch 12000] loss: 0.917\n",
      "Epoch 19 completed in 28.66 seconds\n",
      "[Epoch 20, Batch  2000] loss: 0.902\n",
      "[Epoch 20, Batch  4000] loss: 0.893\n",
      "[Epoch 20, Batch  6000] loss: 0.874\n",
      "[Epoch 20, Batch  8000] loss: 0.876\n",
      "[Epoch 20, Batch 10000] loss: 0.895\n",
      "[Epoch 20, Batch 12000] loss: 0.906\n",
      "Epoch 20 completed in 37.10 seconds\n",
      "Finished Training LR_0.0001\n",
      "Model LR_0.0001 saved to ./data/cifar_LR_0.0001.pth\n",
      "\n",
      "Evaluating LR_0.0001...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for LR_0.0001\n",
      "============================================================\n",
      "Total Training Time: 654.47 seconds\n",
      "Average Epoch Time: 32.72 seconds\n",
      "Overall Test Accuracy: 63.21%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  72.2%\n",
      "  car       :  77.8%\n",
      "  bird      :  47.6%\n",
      "  cat       :  49.6%\n",
      "  deer      :  38.6%\n",
      "  dog       :  52.0%\n",
      "  frog      :  75.1%\n",
      "  horse     :  78.6%\n",
      "  ship      :  77.3%\n",
      "  truck     :  63.3%\n",
      "============================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Training: LR_0.01\n",
      "################################################################################\n",
      "\n",
      "Training LR_0.01...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.067\n",
      "[Epoch  1, Batch  4000] loss: 1.961\n",
      "[Epoch  1, Batch  6000] loss: 1.952\n",
      "[Epoch  1, Batch  8000] loss: 1.957\n",
      "[Epoch  1, Batch 10000] loss: 1.938\n",
      "[Epoch  1, Batch 12000] loss: 1.944\n",
      "Epoch 1 completed in 35.11 seconds\n",
      "[Epoch  2, Batch  2000] loss: 1.952\n",
      "[Epoch  2, Batch  4000] loss: 1.951\n",
      "[Epoch  2, Batch  6000] loss: 1.943\n",
      "[Epoch  2, Batch  8000] loss: 1.952\n",
      "[Epoch  2, Batch 10000] loss: 1.964\n",
      "[Epoch  2, Batch 12000] loss: 1.972\n",
      "Epoch 2 completed in 28.02 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.982\n",
      "[Epoch  3, Batch  4000] loss: 1.981\n",
      "[Epoch  3, Batch  6000] loss: 1.963\n",
      "[Epoch  3, Batch  8000] loss: 1.973\n",
      "[Epoch  3, Batch 10000] loss: 1.965\n",
      "[Epoch  3, Batch 12000] loss: 1.954\n",
      "Epoch 3 completed in 24.34 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.971\n",
      "[Epoch  4, Batch  4000] loss: 1.954\n",
      "[Epoch  4, Batch  6000] loss: 2.013\n",
      "[Epoch  4, Batch  8000] loss: 2.038\n",
      "[Epoch  4, Batch 10000] loss: 1.960\n",
      "[Epoch  4, Batch 12000] loss: 2.006\n",
      "Epoch 4 completed in 25.17 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.973\n",
      "[Epoch  5, Batch  4000] loss: 1.994\n",
      "[Epoch  5, Batch  6000] loss: 1.986\n",
      "[Epoch  5, Batch  8000] loss: 1.986\n",
      "[Epoch  5, Batch 10000] loss: 1.978\n",
      "[Epoch  5, Batch 12000] loss: 1.987\n",
      "Epoch 5 completed in 29.76 seconds\n",
      "[Epoch  6, Batch  2000] loss: 1.952\n",
      "[Epoch  6, Batch  4000] loss: 2.017\n",
      "[Epoch  6, Batch  6000] loss: 1.991\n",
      "[Epoch  6, Batch  8000] loss: 1.987\n",
      "[Epoch  6, Batch 10000] loss: 1.975\n",
      "[Epoch  6, Batch 12000] loss: 1.984\n",
      "Epoch 6 completed in 34.11 seconds\n",
      "[Epoch  7, Batch  2000] loss: 1.981\n",
      "[Epoch  7, Batch  4000] loss: 1.963\n",
      "[Epoch  7, Batch  6000] loss: 1.968\n",
      "[Epoch  7, Batch  8000] loss: 1.988\n",
      "[Epoch  7, Batch 10000] loss: 1.971\n",
      "[Epoch  7, Batch 12000] loss: 1.981\n",
      "Epoch 7 completed in 24.67 seconds\n",
      "[Epoch  8, Batch  2000] loss: 1.983\n",
      "[Epoch  8, Batch  4000] loss: 1.958\n",
      "[Epoch  8, Batch  6000] loss: 1.979\n",
      "[Epoch  8, Batch  8000] loss: 1.977\n",
      "[Epoch  8, Batch 10000] loss: 1.978\n",
      "[Epoch  8, Batch 12000] loss: 1.982\n",
      "Epoch 8 completed in 30.41 seconds\n",
      "[Epoch  9, Batch  2000] loss: 1.990\n",
      "[Epoch  9, Batch  4000] loss: 1.977\n",
      "[Epoch  9, Batch  6000] loss: 1.994\n",
      "[Epoch  9, Batch  8000] loss: 2.000\n",
      "[Epoch  9, Batch 10000] loss: 1.987\n",
      "[Epoch  9, Batch 12000] loss: 2.034\n",
      "Epoch 9 completed in 41.30 seconds\n",
      "[Epoch 10, Batch  2000] loss: 2.008\n",
      "[Epoch 10, Batch  4000] loss: 1.981\n",
      "[Epoch 10, Batch  6000] loss: 1.972\n",
      "[Epoch 10, Batch  8000] loss: 1.998\n",
      "[Epoch 10, Batch 10000] loss: 1.987\n",
      "[Epoch 10, Batch 12000] loss: 1.955\n",
      "Epoch 10 completed in 28.43 seconds\n",
      "[Epoch 11, Batch  2000] loss: 1.995\n",
      "[Epoch 11, Batch  4000] loss: 1.979\n",
      "[Epoch 11, Batch  6000] loss: 1.970\n",
      "[Epoch 11, Batch  8000] loss: 1.985\n",
      "[Epoch 11, Batch 10000] loss: 1.986\n",
      "[Epoch 11, Batch 12000] loss: 2.017\n",
      "Epoch 11 completed in 24.73 seconds\n",
      "[Epoch 12, Batch  2000] loss: 2.070\n",
      "[Epoch 12, Batch  4000] loss: 2.064\n",
      "[Epoch 12, Batch  6000] loss: 2.033\n",
      "[Epoch 12, Batch  8000] loss: 1.990\n",
      "[Epoch 12, Batch 10000] loss: 2.010\n",
      "[Epoch 12, Batch 12000] loss: 1.986\n",
      "Epoch 12 completed in 34.02 seconds\n",
      "[Epoch 13, Batch  2000] loss: 1.976\n",
      "[Epoch 13, Batch  4000] loss: 2.005\n",
      "[Epoch 13, Batch  6000] loss: 2.029\n",
      "[Epoch 13, Batch  8000] loss: 2.013\n",
      "[Epoch 13, Batch 10000] loss: 1.994\n",
      "[Epoch 13, Batch 12000] loss: 1.986\n",
      "Epoch 13 completed in 29.16 seconds\n",
      "[Epoch 14, Batch  2000] loss: 1.964\n",
      "[Epoch 14, Batch  4000] loss: 1.987\n",
      "[Epoch 14, Batch  6000] loss: 1.993\n",
      "[Epoch 14, Batch  8000] loss: 2.030\n",
      "[Epoch 14, Batch 10000] loss: 2.021\n",
      "[Epoch 14, Batch 12000] loss: 2.017\n",
      "Epoch 14 completed in 30.23 seconds\n",
      "[Epoch 15, Batch  2000] loss: 2.015\n",
      "[Epoch 15, Batch  4000] loss: 2.013\n",
      "[Epoch 15, Batch  6000] loss: 1.996\n",
      "[Epoch 15, Batch  8000] loss: 2.003\n",
      "[Epoch 15, Batch 10000] loss: 1.989\n",
      "[Epoch 15, Batch 12000] loss: 1.999\n",
      "Epoch 15 completed in 28.87 seconds\n",
      "[Epoch 16, Batch  2000] loss: 2.013\n",
      "[Epoch 16, Batch  4000] loss: 2.023\n",
      "[Epoch 16, Batch  6000] loss: 2.000\n",
      "[Epoch 16, Batch  8000] loss: 1.978\n",
      "[Epoch 16, Batch 10000] loss: 1.994\n",
      "[Epoch 16, Batch 12000] loss: 1.988\n",
      "Epoch 16 completed in 33.18 seconds\n",
      "[Epoch 17, Batch  2000] loss: 2.008\n",
      "[Epoch 17, Batch  4000] loss: 2.032\n",
      "[Epoch 17, Batch  6000] loss: 2.005\n",
      "[Epoch 17, Batch  8000] loss: 1.995\n",
      "[Epoch 17, Batch 10000] loss: 2.024\n",
      "[Epoch 17, Batch 12000] loss: 2.023\n",
      "Epoch 17 completed in 35.49 seconds\n",
      "[Epoch 18, Batch  2000] loss: 1.985\n",
      "[Epoch 18, Batch  4000] loss: 2.004\n",
      "[Epoch 18, Batch  6000] loss: 1.970\n",
      "[Epoch 18, Batch  8000] loss: 1.990\n",
      "[Epoch 18, Batch 10000] loss: 2.027\n",
      "[Epoch 18, Batch 12000] loss: 2.017\n",
      "Epoch 18 completed in 44.92 seconds\n",
      "[Epoch 19, Batch  2000] loss: 1.988\n",
      "[Epoch 19, Batch  4000] loss: 1.965\n",
      "[Epoch 19, Batch  6000] loss: 1.977\n",
      "[Epoch 19, Batch  8000] loss: 1.996\n",
      "[Epoch 19, Batch 10000] loss: 2.027\n",
      "[Epoch 19, Batch 12000] loss: 1.980\n",
      "Epoch 19 completed in 33.14 seconds\n",
      "[Epoch 20, Batch  2000] loss: 2.000\n",
      "[Epoch 20, Batch  4000] loss: 2.004\n",
      "[Epoch 20, Batch  6000] loss: 2.005\n",
      "[Epoch 20, Batch  8000] loss: 1.972\n",
      "[Epoch 20, Batch 10000] loss: 1.971\n",
      "[Epoch 20, Batch 12000] loss: 1.989\n",
      "Epoch 20 completed in 32.83 seconds\n",
      "Finished Training LR_0.01\n",
      "Model LR_0.01 saved to ./data/cifar_LR_0.01.pth\n",
      "\n",
      "Evaluating LR_0.01...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for LR_0.01\n",
      "============================================================\n",
      "Total Training Time: 627.89 seconds\n",
      "Average Epoch Time: 31.39 seconds\n",
      "Overall Test Accuracy: 24.78%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  53.4%\n",
      "  car       :  47.9%\n",
      "  bird      :   0.3%\n",
      "  cat       :  11.5%\n",
      "  deer      :   0.4%\n",
      "  dog       :   0.8%\n",
      "  frog      :  68.0%\n",
      "  horse     :  53.6%\n",
      "  ship      :   7.9%\n",
      "  truck     :   4.0%\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LR_0.0001/test_accuracy_bird</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_car</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_cat</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_deer</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_dog</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_frog</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_horse</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_plane</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_ship</td><td>▁</td></tr><tr><td>LR_0.0001/test_accuracy_truck</td><td>▁</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LR_0.0001/test_accuracy_bird</td><td>47.6</td></tr><tr><td>LR_0.0001/test_accuracy_car</td><td>77.8</td></tr><tr><td>LR_0.0001/test_accuracy_cat</td><td>49.6</td></tr><tr><td>LR_0.0001/test_accuracy_deer</td><td>38.6</td></tr><tr><td>LR_0.0001/test_accuracy_dog</td><td>52</td></tr><tr><td>LR_0.0001/test_accuracy_frog</td><td>75.1</td></tr><tr><td>LR_0.0001/test_accuracy_horse</td><td>78.6</td></tr><tr><td>LR_0.0001/test_accuracy_plane</td><td>72.2</td></tr><tr><td>LR_0.0001/test_accuracy_ship</td><td>77.3</td></tr><tr><td>LR_0.0001/test_accuracy_truck</td><td>63.3</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">learning_rate_comparison</strong> at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/ue9jy7cy' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/ue9jy7cy</a><br> View project at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251031_215633-ue9jy7cy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "EXPERIMENT 2: OPTIMIZER COMPARISON\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ohsono/jupyterlab/wandb/run-20251031_222841-cg7wt387</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cg7wt387' target=\"_blank\">optimizer_comparison</a></strong> to <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cg7wt387' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cg7wt387</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Optimizers:\n",
      "  - Baseline: SGD with momentum=0.9\n",
      "  - Variant1: SGD without momentum\n",
      "  - Variant2: Adam\n",
      "\n",
      "################################################################################\n",
      "Training: SGD_NoMomentum\n",
      "################################################################################\n",
      "\n",
      "Training SGD_NoMomentum...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 2.304\n",
      "[Epoch  1, Batch  4000] loss: 2.301\n",
      "[Epoch  1, Batch  6000] loss: 2.296\n",
      "[Epoch  1, Batch  8000] loss: 2.287\n",
      "[Epoch  1, Batch 10000] loss: 2.251\n",
      "[Epoch  1, Batch 12000] loss: 2.135\n",
      "Epoch 1 completed in 21.74 seconds\n",
      "[Epoch  2, Batch  2000] loss: 2.041\n",
      "[Epoch  2, Batch  4000] loss: 1.987\n",
      "[Epoch  2, Batch  6000] loss: 1.946\n",
      "[Epoch  2, Batch  8000] loss: 1.919\n",
      "[Epoch  2, Batch 10000] loss: 1.884\n",
      "[Epoch  2, Batch 12000] loss: 1.845\n",
      "Epoch 2 completed in 19.95 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.776\n",
      "[Epoch  3, Batch  4000] loss: 1.732\n",
      "[Epoch  3, Batch  6000] loss: 1.716\n",
      "[Epoch  3, Batch  8000] loss: 1.663\n",
      "[Epoch  3, Batch 10000] loss: 1.615\n",
      "[Epoch  3, Batch 12000] loss: 1.611\n",
      "Epoch 3 completed in 27.96 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.597\n",
      "[Epoch  4, Batch  4000] loss: 1.566\n",
      "[Epoch  4, Batch  6000] loss: 1.543\n",
      "[Epoch  4, Batch  8000] loss: 1.527\n",
      "[Epoch  4, Batch 10000] loss: 1.537\n",
      "[Epoch  4, Batch 12000] loss: 1.486\n",
      "Epoch 4 completed in 16.74 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.491\n",
      "[Epoch  5, Batch  4000] loss: 1.463\n",
      "[Epoch  5, Batch  6000] loss: 1.447\n",
      "[Epoch  5, Batch  8000] loss: 1.441\n",
      "[Epoch  5, Batch 10000] loss: 1.420\n",
      "[Epoch  5, Batch 12000] loss: 1.445\n",
      "Epoch 5 completed in 25.52 seconds\n",
      "[Epoch  6, Batch  2000] loss: 1.391\n",
      "[Epoch  6, Batch  4000] loss: 1.385\n",
      "[Epoch  6, Batch  6000] loss: 1.392\n",
      "[Epoch  6, Batch  8000] loss: 1.356\n",
      "[Epoch  6, Batch 10000] loss: 1.353\n",
      "[Epoch  6, Batch 12000] loss: 1.362\n",
      "Epoch 6 completed in 36.74 seconds\n",
      "[Epoch  7, Batch  2000] loss: 1.335\n",
      "[Epoch  7, Batch  4000] loss: 1.340\n",
      "[Epoch  7, Batch  6000] loss: 1.323\n",
      "[Epoch  7, Batch  8000] loss: 1.308\n",
      "[Epoch  7, Batch 10000] loss: 1.297\n",
      "[Epoch  7, Batch 12000] loss: 1.290\n",
      "Epoch 7 completed in 29.70 seconds\n",
      "[Epoch  8, Batch  2000] loss: 1.271\n",
      "[Epoch  8, Batch  4000] loss: 1.263\n",
      "[Epoch  8, Batch  6000] loss: 1.272\n",
      "[Epoch  8, Batch  8000] loss: 1.266\n",
      "[Epoch  8, Batch 10000] loss: 1.251\n",
      "[Epoch  8, Batch 12000] loss: 1.237\n",
      "Epoch 8 completed in 26.37 seconds\n",
      "[Epoch  9, Batch  2000] loss: 1.237\n",
      "[Epoch  9, Batch  4000] loss: 1.220\n",
      "[Epoch  9, Batch  6000] loss: 1.208\n",
      "[Epoch  9, Batch  8000] loss: 1.189\n",
      "[Epoch  9, Batch 10000] loss: 1.196\n",
      "[Epoch  9, Batch 12000] loss: 1.211\n",
      "Epoch 9 completed in 28.59 seconds\n",
      "[Epoch 10, Batch  2000] loss: 1.201\n",
      "[Epoch 10, Batch  4000] loss: 1.164\n",
      "[Epoch 10, Batch  6000] loss: 1.170\n",
      "[Epoch 10, Batch  8000] loss: 1.152\n",
      "[Epoch 10, Batch 10000] loss: 1.161\n",
      "[Epoch 10, Batch 12000] loss: 1.171\n",
      "Epoch 10 completed in 29.32 seconds\n",
      "[Epoch 11, Batch  2000] loss: 1.142\n",
      "[Epoch 11, Batch  4000] loss: 1.126\n",
      "[Epoch 11, Batch  6000] loss: 1.131\n",
      "[Epoch 11, Batch  8000] loss: 1.125\n",
      "[Epoch 11, Batch 10000] loss: 1.125\n",
      "[Epoch 11, Batch 12000] loss: 1.127\n",
      "Epoch 11 completed in 33.34 seconds\n",
      "[Epoch 12, Batch  2000] loss: 1.086\n",
      "[Epoch 12, Batch  4000] loss: 1.113\n",
      "[Epoch 12, Batch  6000] loss: 1.084\n",
      "[Epoch 12, Batch  8000] loss: 1.105\n",
      "[Epoch 12, Batch 10000] loss: 1.087\n",
      "[Epoch 12, Batch 12000] loss: 1.091\n",
      "Epoch 12 completed in 38.65 seconds\n",
      "[Epoch 13, Batch  2000] loss: 1.058\n",
      "[Epoch 13, Batch  4000] loss: 1.087\n",
      "[Epoch 13, Batch  6000] loss: 1.063\n",
      "[Epoch 13, Batch  8000] loss: 1.060\n",
      "[Epoch 13, Batch 10000] loss: 1.053\n",
      "[Epoch 13, Batch 12000] loss: 1.067\n",
      "Epoch 13 completed in 28.11 seconds\n",
      "[Epoch 14, Batch  2000] loss: 1.041\n",
      "[Epoch 14, Batch  4000] loss: 1.019\n",
      "[Epoch 14, Batch  6000] loss: 1.044\n",
      "[Epoch 14, Batch  8000] loss: 1.026\n",
      "[Epoch 14, Batch 10000] loss: 1.044\n",
      "[Epoch 14, Batch 12000] loss: 1.022\n",
      "Epoch 14 completed in 32.19 seconds\n",
      "[Epoch 15, Batch  2000] loss: 1.013\n",
      "[Epoch 15, Batch  4000] loss: 0.994\n",
      "[Epoch 15, Batch  6000] loss: 0.997\n",
      "[Epoch 15, Batch  8000] loss: 1.003\n",
      "[Epoch 15, Batch 10000] loss: 1.016\n",
      "[Epoch 15, Batch 12000] loss: 1.018\n",
      "Epoch 15 completed in 32.24 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.975\n",
      "[Epoch 16, Batch  4000] loss: 0.979\n",
      "[Epoch 16, Batch  6000] loss: 0.993\n",
      "[Epoch 16, Batch  8000] loss: 0.968\n",
      "[Epoch 16, Batch 10000] loss: 0.980\n",
      "[Epoch 16, Batch 12000] loss: 0.986\n",
      "Epoch 16 completed in 37.18 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.961\n",
      "[Epoch 17, Batch  4000] loss: 0.951\n",
      "[Epoch 17, Batch  6000] loss: 0.973\n",
      "[Epoch 17, Batch  8000] loss: 0.948\n",
      "[Epoch 17, Batch 10000] loss: 0.956\n",
      "[Epoch 17, Batch 12000] loss: 0.963\n",
      "Epoch 17 completed in 29.59 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.924\n",
      "[Epoch 18, Batch  4000] loss: 0.923\n",
      "[Epoch 18, Batch  6000] loss: 0.939\n",
      "[Epoch 18, Batch  8000] loss: 0.955\n",
      "[Epoch 18, Batch 10000] loss: 0.932\n",
      "[Epoch 18, Batch 12000] loss: 0.942\n",
      "Epoch 18 completed in 30.96 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.900\n",
      "[Epoch 19, Batch  4000] loss: 0.907\n",
      "[Epoch 19, Batch  6000] loss: 0.915\n",
      "[Epoch 19, Batch  8000] loss: 0.913\n",
      "[Epoch 19, Batch 10000] loss: 0.923\n",
      "[Epoch 19, Batch 12000] loss: 0.924\n",
      "Epoch 19 completed in 35.15 seconds\n",
      "[Epoch 20, Batch  2000] loss: 0.880\n",
      "[Epoch 20, Batch  4000] loss: 0.898\n",
      "[Epoch 20, Batch  6000] loss: 0.889\n",
      "[Epoch 20, Batch  8000] loss: 0.885\n",
      "[Epoch 20, Batch 10000] loss: 0.915\n",
      "[Epoch 20, Batch 12000] loss: 0.887\n",
      "Epoch 20 completed in 33.76 seconds\n",
      "Finished Training SGD_NoMomentum\n",
      "Model SGD_NoMomentum saved to ./data/cifar_SGD_NoMomentum.pth\n",
      "\n",
      "Evaluating SGD_NoMomentum...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for SGD_NoMomentum\n",
      "============================================================\n",
      "Total Training Time: 593.79 seconds\n",
      "Average Epoch Time: 29.69 seconds\n",
      "Overall Test Accuracy: 63.83%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  63.0%\n",
      "  car       :  81.6%\n",
      "  bird      :  54.4%\n",
      "  cat       :  52.1%\n",
      "  deer      :  49.1%\n",
      "  dog       :  44.6%\n",
      "  frog      :  78.9%\n",
      "  horse     :  67.9%\n",
      "  ship      :  82.4%\n",
      "  truck     :  64.3%\n",
      "============================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Training: Adam_Optimizer\n",
      "################################################################################\n",
      "\n",
      "Training Adam_Optimizer...\n",
      "============================================================\n",
      "[Epoch  1, Batch  2000] loss: 1.908\n",
      "[Epoch  1, Batch  4000] loss: 1.665\n",
      "[Epoch  1, Batch  6000] loss: 1.583\n",
      "[Epoch  1, Batch  8000] loss: 1.512\n",
      "[Epoch  1, Batch 10000] loss: 1.451\n",
      "[Epoch  1, Batch 12000] loss: 1.443\n",
      "Epoch 1 completed in 40.29 seconds\n",
      "[Epoch  2, Batch  2000] loss: 1.357\n",
      "[Epoch  2, Batch  4000] loss: 1.345\n",
      "[Epoch  2, Batch  6000] loss: 1.338\n",
      "[Epoch  2, Batch  8000] loss: 1.338\n",
      "[Epoch  2, Batch 10000] loss: 1.321\n",
      "[Epoch  2, Batch 12000] loss: 1.297\n",
      "Epoch 2 completed in 38.38 seconds\n",
      "[Epoch  3, Batch  2000] loss: 1.243\n",
      "[Epoch  3, Batch  4000] loss: 1.234\n",
      "[Epoch  3, Batch  6000] loss: 1.215\n",
      "[Epoch  3, Batch  8000] loss: 1.229\n",
      "[Epoch  3, Batch 10000] loss: 1.222\n",
      "[Epoch  3, Batch 12000] loss: 1.211\n",
      "Epoch 3 completed in 35.23 seconds\n",
      "[Epoch  4, Batch  2000] loss: 1.152\n",
      "[Epoch  4, Batch  4000] loss: 1.150\n",
      "[Epoch  4, Batch  6000] loss: 1.145\n",
      "[Epoch  4, Batch  8000] loss: 1.157\n",
      "[Epoch  4, Batch 10000] loss: 1.186\n",
      "[Epoch  4, Batch 12000] loss: 1.153\n",
      "Epoch 4 completed in 23.97 seconds\n",
      "[Epoch  5, Batch  2000] loss: 1.082\n",
      "[Epoch  5, Batch  4000] loss: 1.088\n",
      "[Epoch  5, Batch  6000] loss: 1.112\n",
      "[Epoch  5, Batch  8000] loss: 1.134\n",
      "[Epoch  5, Batch 10000] loss: 1.087\n",
      "[Epoch  5, Batch 12000] loss: 1.120\n",
      "Epoch 5 completed in 24.37 seconds\n",
      "[Epoch  6, Batch  2000] loss: 1.054\n",
      "[Epoch  6, Batch  4000] loss: 1.080\n",
      "[Epoch  6, Batch  6000] loss: 1.054\n",
      "[Epoch  6, Batch  8000] loss: 1.073\n",
      "[Epoch  6, Batch 10000] loss: 1.058\n",
      "[Epoch  6, Batch 12000] loss: 1.072\n",
      "Epoch 6 completed in 23.46 seconds\n",
      "[Epoch  7, Batch  2000] loss: 0.998\n",
      "[Epoch  7, Batch  4000] loss: 1.016\n",
      "[Epoch  7, Batch  6000] loss: 1.007\n",
      "[Epoch  7, Batch  8000] loss: 1.042\n",
      "[Epoch  7, Batch 10000] loss: 1.052\n",
      "[Epoch  7, Batch 12000] loss: 1.072\n",
      "Epoch 7 completed in 37.65 seconds\n",
      "[Epoch  8, Batch  2000] loss: 0.985\n",
      "[Epoch  8, Batch  4000] loss: 0.989\n",
      "[Epoch  8, Batch  6000] loss: 1.009\n",
      "[Epoch  8, Batch  8000] loss: 1.022\n",
      "[Epoch  8, Batch 10000] loss: 1.021\n",
      "[Epoch  8, Batch 12000] loss: 1.022\n",
      "Epoch 8 completed in 41.30 seconds\n",
      "[Epoch  9, Batch  2000] loss: 0.956\n",
      "[Epoch  9, Batch  4000] loss: 0.947\n",
      "[Epoch  9, Batch  6000] loss: 0.976\n",
      "[Epoch  9, Batch  8000] loss: 0.998\n",
      "[Epoch  9, Batch 10000] loss: 0.984\n",
      "[Epoch  9, Batch 12000] loss: 1.014\n",
      "Epoch 9 completed in 30.29 seconds\n",
      "[Epoch 10, Batch  2000] loss: 0.919\n",
      "[Epoch 10, Batch  4000] loss: 0.931\n",
      "[Epoch 10, Batch  6000] loss: 0.952\n",
      "[Epoch 10, Batch  8000] loss: 0.976\n",
      "[Epoch 10, Batch 10000] loss: 0.974\n",
      "[Epoch 10, Batch 12000] loss: 0.995\n",
      "Epoch 10 completed in 36.17 seconds\n",
      "[Epoch 11, Batch  2000] loss: 0.916\n",
      "[Epoch 11, Batch  4000] loss: 0.897\n",
      "[Epoch 11, Batch  6000] loss: 0.952\n",
      "[Epoch 11, Batch  8000] loss: 0.953\n",
      "[Epoch 11, Batch 10000] loss: 0.966\n",
      "[Epoch 11, Batch 12000] loss: 0.970\n",
      "Epoch 11 completed in 29.72 seconds\n",
      "[Epoch 12, Batch  2000] loss: 0.873\n",
      "[Epoch 12, Batch  4000] loss: 0.923\n",
      "[Epoch 12, Batch  6000] loss: 0.921\n",
      "[Epoch 12, Batch  8000] loss: 0.925\n",
      "[Epoch 12, Batch 10000] loss: 0.931\n",
      "[Epoch 12, Batch 12000] loss: 0.977\n",
      "Epoch 12 completed in 35.92 seconds\n",
      "[Epoch 13, Batch  2000] loss: 0.870\n",
      "[Epoch 13, Batch  4000] loss: 0.901\n",
      "[Epoch 13, Batch  6000] loss: 0.911\n",
      "[Epoch 13, Batch  8000] loss: 0.907\n",
      "[Epoch 13, Batch 10000] loss: 0.923\n",
      "[Epoch 13, Batch 12000] loss: 0.928\n",
      "Epoch 13 completed in 31.84 seconds\n",
      "[Epoch 14, Batch  2000] loss: 0.829\n",
      "[Epoch 14, Batch  4000] loss: 0.900\n",
      "[Epoch 14, Batch  6000] loss: 0.900\n",
      "[Epoch 14, Batch  8000] loss: 0.906\n",
      "[Epoch 14, Batch 10000] loss: 0.897\n",
      "[Epoch 14, Batch 12000] loss: 0.945\n",
      "Epoch 14 completed in 29.57 seconds\n",
      "[Epoch 15, Batch  2000] loss: 0.857\n",
      "[Epoch 15, Batch  4000] loss: 0.851\n",
      "[Epoch 15, Batch  6000] loss: 0.884\n",
      "[Epoch 15, Batch  8000] loss: 0.896\n",
      "[Epoch 15, Batch 10000] loss: 0.926\n",
      "[Epoch 15, Batch 12000] loss: 0.908\n",
      "Epoch 15 completed in 36.38 seconds\n",
      "[Epoch 16, Batch  2000] loss: 0.839\n",
      "[Epoch 16, Batch  4000] loss: 0.848\n",
      "[Epoch 16, Batch  6000] loss: 0.876\n",
      "[Epoch 16, Batch  8000] loss: 0.868\n",
      "[Epoch 16, Batch 10000] loss: 0.883\n",
      "[Epoch 16, Batch 12000] loss: 0.944\n",
      "Epoch 16 completed in 41.25 seconds\n",
      "[Epoch 17, Batch  2000] loss: 0.836\n",
      "[Epoch 17, Batch  4000] loss: 0.846\n",
      "[Epoch 17, Batch  6000] loss: 0.848\n",
      "[Epoch 17, Batch  8000] loss: 0.859\n",
      "[Epoch 17, Batch 10000] loss: 0.903\n",
      "[Epoch 17, Batch 12000] loss: 0.899\n",
      "Epoch 17 completed in 33.57 seconds\n",
      "[Epoch 18, Batch  2000] loss: 0.824\n",
      "[Epoch 18, Batch  4000] loss: 0.820\n",
      "[Epoch 18, Batch  6000] loss: 0.842\n",
      "[Epoch 18, Batch  8000] loss: 0.864\n",
      "[Epoch 18, Batch 10000] loss: 0.877\n",
      "[Epoch 18, Batch 12000] loss: 0.865\n",
      "Epoch 18 completed in 30.18 seconds\n",
      "[Epoch 19, Batch  2000] loss: 0.799\n",
      "[Epoch 19, Batch  4000] loss: 0.821\n",
      "[Epoch 19, Batch  6000] loss: 0.887\n",
      "[Epoch 19, Batch  8000] loss: 0.830\n",
      "[Epoch 19, Batch 10000] loss: 0.862\n",
      "[Epoch 19, Batch 12000] loss: 0.875\n",
      "Epoch 19 completed in 29.25 seconds\n",
      "[Epoch 20, Batch  2000] loss: 0.770\n",
      "[Epoch 20, Batch  4000] loss: 0.814\n",
      "[Epoch 20, Batch  6000] loss: 0.838\n",
      "[Epoch 20, Batch  8000] loss: 0.840\n",
      "[Epoch 20, Batch 10000] loss: 0.873\n",
      "[Epoch 20, Batch 12000] loss: 0.866\n",
      "Epoch 20 completed in 26.51 seconds\n",
      "Finished Training Adam_Optimizer\n",
      "Model Adam_Optimizer saved to ./data/cifar_Adam_Optimizer.pth\n",
      "\n",
      "Evaluating Adam_Optimizer...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for Adam_Optimizer\n",
      "============================================================\n",
      "Total Training Time: 655.31 seconds\n",
      "Average Epoch Time: 32.77 seconds\n",
      "Overall Test Accuracy: 59.58%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  66.4%\n",
      "  car       :  77.0%\n",
      "  bird      :  41.3%\n",
      "  cat       :  43.8%\n",
      "  deer      :  51.7%\n",
      "  dog       :  49.9%\n",
      "  frog      :  79.3%\n",
      "  horse     :  47.5%\n",
      "  ship      :  70.2%\n",
      "  truck     :  68.7%\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Adam_Optimizer/test_accuracy_bird</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_car</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_cat</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_deer</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_dog</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_frog</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_horse</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_plane</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_ship</td><td>▁</td></tr><tr><td>Adam_Optimizer/test_accuracy_truck</td><td>▁</td></tr><tr><td>+15</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Adam_Optimizer/test_accuracy_bird</td><td>41.3</td></tr><tr><td>Adam_Optimizer/test_accuracy_car</td><td>77</td></tr><tr><td>Adam_Optimizer/test_accuracy_cat</td><td>43.8</td></tr><tr><td>Adam_Optimizer/test_accuracy_deer</td><td>51.7</td></tr><tr><td>Adam_Optimizer/test_accuracy_dog</td><td>49.9</td></tr><tr><td>Adam_Optimizer/test_accuracy_frog</td><td>79.3</td></tr><tr><td>Adam_Optimizer/test_accuracy_horse</td><td>47.5</td></tr><tr><td>Adam_Optimizer/test_accuracy_plane</td><td>66.4</td></tr><tr><td>Adam_Optimizer/test_accuracy_ship</td><td>70.2</td></tr><tr><td>Adam_Optimizer/test_accuracy_truck</td><td>68.7</td></tr><tr><td>+15</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">optimizer_comparison</strong> at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cg7wt387' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cg7wt387</a><br> View project at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251031_222841-cg7wt387/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "EXPERIMENT 3: BATCH SIZE COMPARISON\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ohsono/jupyterlab/wandb/run-20251031_224951-cltp2g35</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cltp2g35' target=\"_blank\">batch_size_comparison</a></strong> to <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cltp2g35' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cltp2g35</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Batch Sizes:\n",
      "  - Baseline: batch_size=4\n",
      "  - Large:    batch_size=32 (8x larger)\n",
      "\n",
      "################################################################################\n",
      "Training: BatchSize_32\n",
      "################################################################################\n",
      "\n",
      "Training BatchSize_32...\n",
      "============================================================\n",
      "Epoch 1 completed in 5.25 seconds\n",
      "Epoch 2 completed in 5.27 seconds\n",
      "Epoch 3 completed in 3.62 seconds\n",
      "Epoch 4 completed in 4.70 seconds\n",
      "Epoch 5 completed in 5.50 seconds\n",
      "Epoch 6 completed in 6.00 seconds\n",
      "Epoch 7 completed in 5.99 seconds\n",
      "Epoch 8 completed in 6.23 seconds\n",
      "Epoch 9 completed in 3.80 seconds\n",
      "Epoch 10 completed in 4.07 seconds\n",
      "Epoch 11 completed in 3.84 seconds\n",
      "Epoch 12 completed in 6.23 seconds\n",
      "Epoch 13 completed in 5.14 seconds\n",
      "Epoch 14 completed in 5.41 seconds\n",
      "Epoch 15 completed in 5.25 seconds\n",
      "Epoch 16 completed in 6.48 seconds\n",
      "Epoch 17 completed in 5.24 seconds\n",
      "Epoch 18 completed in 4.04 seconds\n",
      "Epoch 19 completed in 3.83 seconds\n",
      "Epoch 20 completed in 4.97 seconds\n",
      "Finished Training BatchSize_32\n",
      "Model BatchSize_32 saved to ./data/cifar_BatchSize_32.pth\n",
      "\n",
      "Evaluating BatchSize_32...\n",
      "\n",
      "============================================================\n",
      "Performance Summary for OriginalNet\n",
      "============================================================\n",
      "Total Training Time: 100.88 seconds\n",
      "Average Epoch Time: 5.04 seconds\n",
      "Overall Test Accuracy: 64.03%\n",
      "\n",
      "Per-Class Accuracies:\n",
      "  plane     :  67.1%\n",
      "  car       :  73.2%\n",
      "  bird      :  47.4%\n",
      "  cat       :  53.2%\n",
      "  deer      :  54.1%\n",
      "  dog       :  46.8%\n",
      "  frog      :  81.3%\n",
      "  horse     :  70.6%\n",
      "  ship      :  78.8%\n",
      "  truck     :  67.8%\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>OriginalNet/test_accuracy_bird</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_car</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_cat</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_deer</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_dog</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_frog</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_horse</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_plane</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_ship</td><td>▁</td></tr><tr><td>OriginalNet/test_accuracy_truck</td><td>▁</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>OriginalNet/test_accuracy_bird</td><td>47.4</td></tr><tr><td>OriginalNet/test_accuracy_car</td><td>73.2</td></tr><tr><td>OriginalNet/test_accuracy_cat</td><td>53.2</td></tr><tr><td>OriginalNet/test_accuracy_deer</td><td>54.1</td></tr><tr><td>OriginalNet/test_accuracy_dog</td><td>46.8</td></tr><tr><td>OriginalNet/test_accuracy_frog</td><td>81.3</td></tr><tr><td>OriginalNet/test_accuracy_horse</td><td>70.6</td></tr><tr><td>OriginalNet/test_accuracy_plane</td><td>67.1</td></tr><tr><td>OriginalNet/test_accuracy_ship</td><td>78.8</td></tr><tr><td>OriginalNet/test_accuracy_truck</td><td>67.8</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">batch_size_comparison</strong> at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cltp2g35' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters/runs/cltp2g35</a><br> View project at: <a href='https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters' target=\"_blank\">https://wandb.ai/ohsono-ucla/STATS413_HW3_Hyperparameters</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251031_224951-cltp2g35/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER COMPARISON - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Learning Rate Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration                  Test Accuracy        Training Time (s)   \n",
      "--------------------------------------------------------------------------------\n",
      "OriginalNet                     60.63%                  620.22\n",
      "LR_0.0001                       63.21%                  654.47\n",
      "LR_0.01                         24.78%                  627.89\n",
      "\n",
      "Optimizer Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration                  Test Accuracy        Training Time (s)   \n",
      "--------------------------------------------------------------------------------\n",
      "SGD_NoMomentum                  63.83%                  593.79\n",
      "Adam_Optimizer                  59.58%                  655.31\n",
      "\n",
      "Batch Size Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration                  Test Accuracy        Training Time (s)   \n",
      "--------------------------------------------------------------------------------\n",
      "BatchSize_32                    64.03%                  100.88\n",
      "\n",
      "Baseline (OriginalNet)         (See Learning Rate)  (See above)         \n",
      "\n",
      "================================================================================\n",
      "Key Observations to Report:\n",
      "1. Learning Rate Impact: Compare convergence speed and final accuracy\n",
      "2. Optimizer Differences: SGD vs Adam optimization paths\n",
      "3. Batch Size Effects: Training stability and generalization\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hyperparameter Comparison Experiments\n",
    "Course: STATS 413 HW3 - Section 0.2\n",
    "\n",
    "This module compares different hyperparameter settings:\n",
    "1. Learning Rate: smaller (0.0001) and larger (0.01) vs baseline (0.001)\n",
    "2. Optimizer: SGD without momentum and Adam vs baseline (SGD with momentum)\n",
    "3. Batch Size: 32 vs baseline (4)\n",
    "\"\"\"\n",
    "\n",
    "# Model variants for hyperparameter experiments\n",
    "class LearningRateNet_Small(CNNModel):\n",
    "    \"\"\"CNN with smaller learning rate (0.0001).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"LR_0.0001\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_optimizer(self, config: TrainingConfig) -> optim.Optimizer:\n",
    "        \"\"\"Override learning rate to 0.0001.\"\"\"\n",
    "        return optim.SGD(self.parameters(), lr=0.0001, momentum=config.momentum)\n",
    "\n",
    "\n",
    "class LearningRateNet_Large(CNNModel):\n",
    "    \"\"\"CNN with larger learning rate (0.01).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"LR_0.01\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_optimizer(self, config: TrainingConfig) -> optim.Optimizer:\n",
    "        \"\"\"Override learning rate to 0.01.\"\"\"\n",
    "        return optim.SGD(self.parameters(), lr=0.01, momentum=config.momentum)\n",
    "\n",
    "\n",
    "class SGDNoMomentumNet(CNNModel):\n",
    "    \"\"\"CNN with SGD optimizer without momentum.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"SGD_NoMomentum\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_optimizer(self, config: TrainingConfig) -> optim.Optimizer:\n",
    "        \"\"\"Override to use SGD without momentum.\"\"\"\n",
    "        return optim.SGD(self.parameters(), lr=config.learning_rate, momentum=0)\n",
    "\n",
    "\n",
    "class AdamNet(CNNModel):\n",
    "    \"\"\"CNN with Adam optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(model_name=\"Adam_Optimizer\")\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_optimizer(self, config: TrainingConfig) -> optim.Optimizer:\n",
    "        \"\"\"Override to use Adam optimizer.\"\"\"\n",
    "        return optim.Adam(self.parameters(), lr=config.learning_rate)\n",
    "\n",
    "\n",
    "def main_hyperparameter_comparison():\n",
    "    \"\"\"\n",
    "    Main function to compare different hyperparameter settings.\n",
    "    \n",
    "    Experiments:\n",
    "    1. Baseline: LR=0.001, SGD with momentum=0.9, batch_size=4\n",
    "    2. Learning Rate variations: 0.0001 (smaller), 0.01 (larger)\n",
    "    3. Optimizer variations: SGD without momentum, Adam\n",
    "    4. Batch Size: 32 (larger)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER COMPARISON EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # ========== EXPERIMENT 1: Learning Rate Comparison ==========\n",
    "    print(\"\\n\" + \"~\"*80)\n",
    "    print(\"EXPERIMENT 1: LEARNING RATE COMPARISON\")\n",
    "    print(\"~\"*80)\n",
    "    \n",
    "    # Initialize wandb for learning rate experiment\n",
    "    wandb.init(\n",
    "        project=\"STATS413_HW3_Hyperparameters\",\n",
    "        entity=\"ohsono-ucla\",\n",
    "        name=\"learning_rate_comparison\",\n",
    "        config={\n",
    "            \"experiment\": \"learning_rate\",\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 4\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    config_baseline = TrainingConfig(learning_rate=0.001, momentum=0.9, epochs=20, batch_size=4)\n",
    "    \n",
    "    print(\"\\nComparing Learning Rates:\")\n",
    "    print(\"  - Baseline: 0.001 (tutorial default)\")\n",
    "    print(\"  - Smaller:  0.0001 (10x smaller)\")\n",
    "    print(\"  - Larger:   0.01 (10x larger)\")\n",
    "    \n",
    "    lr_models = [\n",
    "        OriginalNet(),           # Baseline: LR=0.001\n",
    "        LearningRateNet_Small(), # LR=0.0001\n",
    "        LearningRateNet_Large()  # LR=0.01\n",
    "    ]\n",
    "    \n",
    "    for model in lr_models:\n",
    "        trainloader, testloader, classes = prepare_data(config_baseline)\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"Training: {model.model_name}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        model.train_model(trainloader, config_baseline, criterion)\n",
    "        model.save_model(f'./data/cifar_{model.model_name}.pth')\n",
    "        model.evaluate_model(testloader, classes, config_baseline)\n",
    "        \n",
    "        all_results.append({\n",
    "            'experiment': 'Learning Rate',\n",
    "            'name': model.model_name,\n",
    "            'accuracy': model.metrics.test_accuracy,\n",
    "            'training_time': model.metrics.total_training_time\n",
    "        })\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # ========== EXPERIMENT 2: Optimizer Comparison ==========\n",
    "    print(\"\\n\" + \"~\"*80)\n",
    "    print(\"EXPERIMENT 2: OPTIMIZER COMPARISON\")\n",
    "    print(\"~\"*80)\n",
    "    \n",
    "    # Initialize wandb for optimizer experiment\n",
    "    wandb.init(\n",
    "        project=\"STATS413_HW3_Hyperparameters\",\n",
    "        entity=\"ohsono-ucla\",\n",
    "        name=\"optimizer_comparison\",\n",
    "        config={\n",
    "            \"experiment\": \"optimizer\",\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 4\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nComparing Optimizers:\")\n",
    "    print(\"  - Baseline: SGD with momentum=0.9\")\n",
    "    print(\"  - Variant1: SGD without momentum\")\n",
    "    print(\"  - Variant2: Adam\")\n",
    "    \n",
    "    optimizer_models = [\n",
    "        SGDNoMomentumNet(),  # SGD without momentum\n",
    "        AdamNet()            # Adam\n",
    "    ]\n",
    "    \n",
    "    for model in optimizer_models:\n",
    "        trainloader, testloader, classes = prepare_data(config_baseline)\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"Training: {model.model_name}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        model.train_model(trainloader, config_baseline, criterion)\n",
    "        model.save_model(f'./data/cifar_{model.model_name}.pth')\n",
    "        model.evaluate_model(testloader, classes, config_baseline)\n",
    "        \n",
    "        all_results.append({\n",
    "            'experiment': 'Optimizer',\n",
    "            'name': model.model_name,\n",
    "            'accuracy': model.metrics.test_accuracy,\n",
    "            'training_time': model.metrics.total_training_time\n",
    "        })\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # ========== EXPERIMENT 3: Batch Size Comparison ==========\n",
    "    print(\"\\n\" + \"~\"*80)\n",
    "    print(\"EXPERIMENT 3: BATCH SIZE COMPARISON\")\n",
    "    print(\"~\"*80)\n",
    "    \n",
    "    # Initialize wandb for batch size experiment\n",
    "    wandb.init(\n",
    "        project=\"STATS413_HW3_Hyperparameters\",\n",
    "        entity=\"ohsono-ucla\",\n",
    "        name=\"batch_size_comparison\",\n",
    "        config={\n",
    "            \"experiment\": \"batch_size\",\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 32\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    config_large_batch = TrainingConfig(learning_rate=0.001, momentum=0.9, epochs=20, batch_size=32)\n",
    "    \n",
    "    print(\"\\nComparing Batch Sizes:\")\n",
    "    print(\"  - Baseline: batch_size=4\")\n",
    "    print(\"  - Large:    batch_size=32 (8x larger)\")\n",
    "    \n",
    "    batch_model = OriginalNet()\n",
    "    batch_model.model_name = \"BatchSize_32\"\n",
    "    \n",
    "    trainloader, testloader, classes = prepare_data(config_large_batch)\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"Training: {batch_model.model_name}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    batch_model.train_model(trainloader, config_large_batch, criterion)\n",
    "    batch_model.save_model(f'./data/cifar_{batch_model.model_name}.pth')\n",
    "    batch_model.evaluate_model(testloader, classes, config_large_batch)\n",
    "    \n",
    "    all_results.append({\n",
    "        'experiment': 'Batch Size',\n",
    "        'name': batch_model.model_name,\n",
    "        'accuracy': batch_model.metrics.test_accuracy,\n",
    "        'training_time': batch_model.metrics.total_training_time\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # ========== FINAL SUMMARY ==========\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER COMPARISON - FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by experiment\n",
    "    experiments = {}\n",
    "    for result in all_results:\n",
    "        exp_name = result['experiment']\n",
    "        if exp_name not in experiments:\n",
    "            experiments[exp_name] = []\n",
    "        experiments[exp_name].append(result)\n",
    "    \n",
    "    for exp_name, results in experiments.items():\n",
    "        print(f\"\\n{exp_name} Comparison:\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        print(f\"{'Configuration':<30} {'Test Accuracy':<20} {'Training Time (s)':<20}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        for result in results:\n",
    "            print(f\"{result['name']:<30} {result['accuracy']:>6.2f}%{'':<13} {result['training_time']:>10.2f}\")\n",
    "    \n",
    "    # Add baseline for reference\n",
    "    print(f\"\\n{'Baseline (OriginalNet)':<30} {'(See Learning Rate)':<20} {'(See above)':<20}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Key Observations to Report:\")\n",
    "    print(\"1. Learning Rate Impact: Compare convergence speed and final accuracy\")\n",
    "    print(\"2. Optimizer Differences: SGD vs Adam optimization paths\")\n",
    "    print(\"3. Batch Size Effects: Training stability and generalization\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_hyperparameter_comparison()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
